{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra√ß√£o de Features - 5 Bra√ßos Experimentais\n",
        "\n",
        "Este notebook extrai features para os 5 bra√ßos experimentais:\n",
        "\n",
        "1. **Baseline CNN:** ResNet/U-Net padr√£o\n",
        "2. **ViT Puro:** ViT pr√©-treinado no ImageNet\n",
        "3. **ViT + Contrastive:** Usando domain_specific_cl\n",
        "4. **ViT + MIM:** Usando MIM-Med3D\n",
        "5. **ViT + Sparse:** Aplica√ß√£o de esparsidade nas features\n",
        "\n",
        "## Estrutura de Sa√≠da\n",
        "\n",
        "As features ser√£o salvas em:\n",
        "```\n",
        "features/\n",
        "‚îú‚îÄ‚îÄ baseline_cnn/\n",
        "‚îú‚îÄ‚îÄ vit_pure/\n",
        "‚îú‚îÄ‚îÄ vit_contrastive/\n",
        "‚îú‚îÄ‚îÄ vit_mim/\n",
        "‚îî‚îÄ‚îÄ vit_sparse/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from transformers import TFAutoModel, AutoFeatureExtractor\n",
        "\n",
        "# ============================================\n",
        "# DETEC√á√ÉO DE AMBIENTE (COLAB OU LOCAL)\n",
        "# ============================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Colab detectado - Drive montado\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚úÖ Ambiente local detectado\")\n",
        "\n",
        "# Configurar caminhos baseado no ambiente\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = Path(\"/content/drive/MyDrive/Mestrado_TCC\")\n",
        "    DATA_DIR = BASE_DIR / \"datasets\" / \"processed\"\n",
        "    FEATURES_DIR = BASE_DIR / \"features\"\n",
        "    # Mudar para diret√≥rio do framework\n",
        "    FRAMEWORK_DIR = BASE_DIR / \"Framework\"\n",
        "    if FRAMEWORK_DIR.exists():\n",
        "        os.chdir(FRAMEWORK_DIR)\n",
        "else:\n",
        "    BASE_DIR = Path(\"../\")\n",
        "    DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
        "    FEATURES_DIR = BASE_DIR / \"features\"\n",
        "\n",
        "# Configura√ß√£o de GPU\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    set_global_policy(policy)\n",
        "    print(\"‚úÖ GPU ativa - Mixed Precision habilitado\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU n√£o dispon√≠vel - Usando CPU\")\n",
        "\n",
        "# Criar diret√≥rios\n",
        "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Criar diret√≥rios para cada bra√ßo experimental\n",
        "EXPERIMENTAL_ARMS = [\n",
        "    \"baseline_cnn\",\n",
        "    \"vit_pure\",\n",
        "    \"vit_contrastive\",\n",
        "    \"vit_mim\",\n",
        "    \"vit_sparse\"\n",
        "]\n",
        "\n",
        "for arm in EXPERIMENTAL_ARMS:\n",
        "    (FEATURES_DIR / arm).mkdir(exist_ok=True)\n",
        "\n",
        "# Batch sizes otimizados por ambiente\n",
        "BATCH_SIZES = {\n",
        "    \"baseline_cnn\": 64 if IN_COLAB else 32,\n",
        "    \"vit_pure\": 32 if IN_COLAB else 16,\n",
        "    \"vit_contrastive\": 16 if IN_COLAB else 8,\n",
        "    \"vit_mim\": 8 if IN_COLAB else 4,\n",
        "    \"vit_sparse\": 64 if IN_COLAB else 32\n",
        "}\n",
        "\n",
        "print(f\"\\nüìÅ Diret√≥rios configurados:\")\n",
        "print(f\"   BASE_DIR: {BASE_DIR}\")\n",
        "print(f\"   FEATURES_DIR: {FEATURES_DIR}\")\n",
        "print(f\"\\nüìä Batch sizes: {BATCH_SIZES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fun√ß√µes Auxiliares para Carregamento de Dados M√©dicos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "\n",
        "def load_medical_image(filepath, normalize=True, slice_idx=None):\n",
        "    \"\"\"\n",
        "    Carrega imagem m√©dica (2D slice de volume 3D ou imagem 2D).\n",
        "    Suporta formatos .nii.gz, .nii, .jpg, .png\n",
        "    \n",
        "    Args:\n",
        "        filepath: Caminho para o arquivo\n",
        "        normalize: Se True, normaliza para [0, 255]\n",
        "        slice_idx: √çndice do slice para volumes 3D (None = slice central)\n",
        "    \"\"\"\n",
        "    filepath = Path(filepath)\n",
        "    \n",
        "    if filepath.suffix == '.gz' or '.nii' in filepath.name:\n",
        "        # Arquivo NIfTI\n",
        "        img = nib.load(str(filepath))\n",
        "        data = img.get_fdata()\n",
        "        \n",
        "        # Se for 3D, pegar slice central ou especificado\n",
        "        if len(data.shape) == 3:\n",
        "            if slice_idx is None:\n",
        "                slice_idx = data.shape[2] // 2\n",
        "            data = data[:, :, slice_idx]\n",
        "        \n",
        "        # Normalizar para [0, 255] e converter para uint8\n",
        "        if normalize:\n",
        "            data_min, data_max = data.min(), data.max()\n",
        "            if data_max > data_min:\n",
        "                data = (data - data_min) / (data_max - data_min + 1e-8) * 255\n",
        "            else:\n",
        "                data = np.zeros_like(data)\n",
        "        \n",
        "        # Converter para RGB (repetir canal se necess√°rio)\n",
        "        if len(data.shape) == 2:\n",
        "            data = np.stack([data, data, data], axis=-1)\n",
        "        \n",
        "        return tf.cast(data, tf.uint8)\n",
        "    else:\n",
        "        # Imagem 2D padr√£o\n",
        "        image = tf.io.read_file(str(filepath))\n",
        "        if filepath.suffix.lower() in ['.jpg', '.jpeg']:\n",
        "            image = tf.image.decode_jpeg(image, channels=3)\n",
        "        elif filepath.suffix.lower() == '.png':\n",
        "            image = tf.image.decode_png(image, channels=3)\n",
        "        return image\n",
        "\n",
        "def preprocess_image(image, input_size=224, method='bilinear'):\n",
        "    \"\"\"Redimensiona e normaliza imagem para o modelo.\"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    if len(image.shape) == 2:\n",
        "        image = tf.expand_dims(image, -1)\n",
        "        image = tf.repeat(image, 3, axis=-1)\n",
        "    \n",
        "    resize_method = tf.image.ResizeMethod.BILINEAR if method == 'bilinear' else tf.image.ResizeMethod.LANCZOS3\n",
        "    image = tf.image.resize(image, [input_size, input_size], method=resize_method)\n",
        "    return image\n",
        "\n",
        "print(\"Fun√ß√µes auxiliares criadas!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bra√ßo 1: Baseline CNN (ResNet50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_baseline_cnn_extractor(input_size=224):\n",
        "    \"\"\"Constr√≥i extrator de features usando ResNet50 pr√©-treinado no ImageNet.\"\"\"\n",
        "    base_model = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(input_size, input_size, 3)\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Global Average Pooling\n",
        "    inputs = layers.Input(shape=(input_size, input_size, 3), dtype=tf.float32)\n",
        "    x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    model = models.Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "print(\"Extrator Baseline CNN criado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bra√ßo 2: ViT Puro (ImageNet pr√©-treinado)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vit_pure_extractor(model_name=\"google/vit-base-patch16-224\", input_size=224):\n",
        "    \"\"\"Constr√≥i extrator de features usando ViT pr√©-treinado no ImageNet.\"\"\"\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "    vit_model = TFAutoModel.from_pretrained(model_name)\n",
        "    vit_model.trainable = False\n",
        "    \n",
        "    def vit_preprocessing(x):\n",
        "        x = tf.cast(x, tf.float32) / 255.0\n",
        "        mean = tf.constant(feature_extractor.image_mean, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "        std = tf.constant(feature_extractor.image_std, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "        return (x - mean) / std\n",
        "    \n",
        "    inputs = layers.Input(shape=(input_size, input_size, 3), dtype=tf.float32)\n",
        "    x = vit_preprocessing(inputs)\n",
        "    \n",
        "    def call_vit(x):\n",
        "        outputs = vit_model(pixel_values=x)\n",
        "        return outputs.last_hidden_state[:, 0]  # token [CLS]\n",
        "    \n",
        "    features = layers.Lambda(call_vit, name=\"vit_features\")(x)\n",
        "    model = models.Model(inputs=inputs, outputs=features)\n",
        "    return model\n",
        "\n",
        "print(\"Extrator ViT Puro criado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bra√ßo 3: ViT + Contrastive (domain_specific_cl)\n",
        "\n",
        "**Nota:** Este reposit√≥rio usa TensorFlow 1.x, que √© incompat√≠vel com vers√µes mais recentes. \n",
        "A integra√ß√£o pode requerer um ambiente separado ou adapta√ß√£o do c√≥digo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vit_contrastive_extractor(repo_path=\"../repositories/domain_specific_cl\", checkpoint_path=None):\n",
        "    \"\"\"\n",
        "    Constr√≥i extrator usando modelo contrastivo do domain_specific_cl.\n",
        "    \n",
        "    IMPORTANTE: Este reposit√≥rio usa TensorFlow 1.x, que requer adapta√ß√£o especial.\n",
        "    \n",
        "    Args:\n",
        "        repo_path: Caminho para o reposit√≥rio clonado\n",
        "        checkpoint_path: Caminho para o checkpoint do modelo pr√©-treinado\n",
        "    \"\"\"\n",
        "    repo_path = Path(repo_path).absolute()\n",
        "    \n",
        "    if not repo_path.exists():\n",
        "        print(\"‚ö†Ô∏è  Reposit√≥rio domain_specific_cl n√£o encontrado!\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "        return build_vit_pure_extractor()\n",
        "    \n",
        "    # Adicionar ao path do Python\n",
        "    sys.path.insert(0, str(repo_path))\n",
        "    \n",
        "    try:\n",
        "        # Tentar importar m√≥dulos do reposit√≥rio\n",
        "        # Nota: Isso pode falhar se TensorFlow 1.x n√£o estiver instalado\n",
        "        import models\n",
        "        import utils\n",
        "        \n",
        "        print(\"‚úÖ M√≥dulos do domain_specific_cl importados com sucesso\")\n",
        "        print(\"‚ö†Ô∏è  Para usar o modelo contrastivo, voc√™ precisa:\")\n",
        "        print(\"   1. Instalar TensorFlow 1.12.0 em um ambiente separado\")\n",
        "        print(\"   2. Treinar ou baixar o modelo pr√©-treinado\")\n",
        "        print(\"   3. Carregar o checkpoint e extrair features do encoder\")\n",
        "        print(\"   Por enquanto, usando ViT puro como fallback\")\n",
        "        \n",
        "        # TODO: Implementar carregamento do modelo quando TensorFlow 1.x estiver dispon√≠vel\n",
        "        # Exemplo:\n",
        "        # cfg = utils.load_config(...)\n",
        "        # model = models.modelObj(cfg)\n",
        "        # encoder = model.encoder_network(...)\n",
        "        # return encoder\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  Erro ao importar m√≥dulos do domain_specific_cl: {e}\")\n",
        "        print(\"   Isso √© esperado se TensorFlow 1.x n√£o estiver instalado\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Erro inesperado: {e}\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "    \n",
        "    # Fallback para ViT puro\n",
        "    return build_vit_pure_extractor()\n",
        "\n",
        "print(\"Extrator ViT + Contrastive (com fallback) criado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bra√ßo 4: ViT + MIM (MIM-Med3D)\n",
        "\n",
        "**Nota:** Este reposit√≥rio usa PyTorch. Vamos criar um wrapper para converter para TensorFlow ou usar diretamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vit_mim_extractor(repo_path=\"../repositories/MIM-Med3D\", checkpoint_path=None, model_type=\"ViTSimMIM\"):\n",
        "    \"\"\"\n",
        "    Constr√≥i extrator usando modelo MIM do MIM-Med3D.\n",
        "    \n",
        "    IMPORTANTE: Este reposit√≥rio usa PyTorch. A integra√ß√£o pode requerer convers√£o ou uso direto do PyTorch.\n",
        "    \n",
        "    Args:\n",
        "        repo_path: Caminho para o reposit√≥rio clonado\n",
        "        checkpoint_path: Caminho para o checkpoint do modelo pr√©-treinado\n",
        "        model_type: Tipo de modelo (\"ViTSimMIM\", \"MAE\", \"VisionTransformer3D\")\n",
        "    \"\"\"\n",
        "    repo_path = Path(repo_path).absolute()\n",
        "    code_path = repo_path / \"code\"\n",
        "    \n",
        "    if not repo_path.exists():\n",
        "        print(\"‚ö†Ô∏è  Reposit√≥rio MIM-Med3D n√£o encontrado!\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "        return build_vit_pure_extractor()\n",
        "    \n",
        "    # Adicionar ao path do Python\n",
        "    sys.path.insert(0, str(code_path))\n",
        "    \n",
        "    try:\n",
        "        # Tentar importar m√≥dulos do reposit√≥rio\n",
        "        import torch\n",
        "        from models import ViTSimMIM, MAE, VisionTransformer3D\n",
        "        \n",
        "        print(\"‚úÖ M√≥dulos do MIM-Med3D importados com sucesso\")\n",
        "        print(\"‚ö†Ô∏è  Para usar o modelo MIM, voc√™ precisa:\")\n",
        "        print(\"   1. Instalar depend√™ncias: pip install -r requirements.txt\")\n",
        "        print(\"   2. Baixar ou treinar o modelo pr√©-treinado\")\n",
        "        print(\"   3. Carregar o checkpoint e extrair features\")\n",
        "        print(\"   Por enquanto, usando ViT puro como fallback\")\n",
        "        \n",
        "        # TODO: Implementar carregamento do modelo quando PyTorch estiver dispon√≠vel\n",
        "        # Exemplo:\n",
        "        # if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        #     checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        #     model = ViTSimMIM(...)\n",
        "        #     model.load_state_dict(checkpoint['state_dict'])\n",
        "        #     model.eval()\n",
        "        #     return model\n",
        "        # else:\n",
        "        #     print(\"Checkpoint n√£o encontrado, usando modelo sem pr√©-treinamento\")\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  Erro ao importar m√≥dulos do MIM-Med3D: {e}\")\n",
        "        print(\"   Verifique se PyTorch e depend√™ncias est√£o instaladas\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Erro inesperado: {e}\")\n",
        "        print(\"   Usando ViT puro como fallback\")\n",
        "    \n",
        "    # Fallback para ViT puro\n",
        "    return build_vit_pure_extractor()\n",
        "\n",
        "print(\"Extrator ViT + MIM (com fallback) criado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bra√ßo 5: ViT + Sparse\n",
        "\n",
        "Aplica esparsidade nas features extra√≠das pelo ViT puro usando Dictionary Learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import DictionaryLearning\n",
        "\n",
        "def apply_sparsity_to_features(features, n_atoms=50, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Aplica esparsidade nas features usando Dictionary Learning.\n",
        "    \n",
        "    Args:\n",
        "        features: Array de features (n_samples, n_features)\n",
        "        n_atoms: N√∫mero de √°tomos no dicion√°rio\n",
        "        alpha: Par√¢metro de regulariza√ß√£o para esparsidade\n",
        "    \"\"\"\n",
        "    # Aprender dicion√°rio\n",
        "    dict_learner = DictionaryLearning(\n",
        "        n_components=n_atoms,\n",
        "        alpha=alpha,\n",
        "        fit_algorithm='lars',\n",
        "        transform_algorithm='lasso_lars',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Aprender dicion√°rio e transformar features\n",
        "    sparse_features = dict_learner.fit_transform(features)\n",
        "    \n",
        "    return sparse_features, dict_learner\n",
        "\n",
        "print(\"Fun√ß√£o de esparsidade criada!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fun√ß√£o de Extra√ß√£o Gen√©rica\n",
        "\n",
        "Esta fun√ß√£o extrai features de um dataset usando qualquer um dos modelos acima.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_from_files(model, file_list, labels_list, output_path, batch_size=32, input_size=224):\n",
        "    \"\"\"\n",
        "    Extrai features de uma lista de arquivos usando um modelo.\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo de extra√ß√£o de features (TensorFlow/Keras)\n",
        "        file_list: Lista de caminhos para arquivos de imagem\n",
        "        labels_list: Lista de labels correspondentes\n",
        "        output_path: Caminho para salvar as features (.npy)\n",
        "        batch_size: Tamanho do batch\n",
        "        input_size: Tamanho da imagem de entrada\n",
        "    \"\"\"\n",
        "    features_list = []\n",
        "    labels_array = []\n",
        "    \n",
        "    print(f\"Extraindo features de {len(file_list)} arquivos...\")\n",
        "    \n",
        "    # Processar em batches\n",
        "    for i in range(0, len(file_list), batch_size):\n",
        "        batch_files = file_list[i:i+batch_size]\n",
        "        batch_labels = labels_list[i:i+batch_size]\n",
        "        \n",
        "        # Carregar e pr√©-processar imagens\n",
        "        batch_images = []\n",
        "        for filepath in batch_files:\n",
        "            try:\n",
        "                image = load_medical_image(filepath, normalize=True)\n",
        "                image = preprocess_image(image, input_size=input_size)\n",
        "                batch_images.append(image)\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao carregar {filepath}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if len(batch_images) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Converter para tensor\n",
        "        batch_images = tf.stack(batch_images)\n",
        "        \n",
        "        # Extrair features\n",
        "        batch_features = model.predict(batch_images, verbose=0)\n",
        "        features_list.append(batch_features)\n",
        "        labels_array.extend(batch_labels[:len(batch_features)])\n",
        "        \n",
        "        if (i + batch_size) % (batch_size * 10) == 0:\n",
        "            print(f\"  Processados {min(i + batch_size, len(file_list))}/{len(file_list)} arquivos...\")\n",
        "    \n",
        "    # Concatenar todas as features\n",
        "    if len(features_list) > 0:\n",
        "        all_features = np.concatenate(features_list, axis=0)\n",
        "        all_labels = np.array(labels_array)\n",
        "        \n",
        "        # Salvar\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        np.save(str(output_path), all_features)\n",
        "        np.save(str(output_path).replace('_features.npy', '_labels.npy'), all_labels)\n",
        "        \n",
        "        print(f\"‚úÖ Features salvas em: {output_path}\")\n",
        "        print(f\"   Shape: {all_features.shape}\")\n",
        "        return all_features, all_labels\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Nenhuma feature foi extra√≠da!\")\n",
        "        return None, None\n",
        "\n",
        "print(\"Fun√ß√£o de extra√ß√£o gen√©rica criada!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exemplo de Uso\n",
        "\n",
        "Aqui est√° um exemplo de como usar as fun√ß√µes acima para extrair features de um dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo: Extrair features com ViT Puro\n",
        "# Descomente e ajuste os caminhos conforme necess√°rio\n",
        "\n",
        "# # 1. Construir modelo\n",
        "# vit_model = build_vit_pure_extractor()\n",
        "# \n",
        "# # 2. Preparar lista de arquivos (exemplo)\n",
        "# # file_list = list((DATA_DIR / \"ACDC\" / \"training\").glob(\"**/*.nii.gz\"))\n",
        "# # labels_list = [0] * len(file_list)  # Ajustar conforme necess√°rio\n",
        "# \n",
        "# # 3. Extrair features\n",
        "# # features, labels = extract_features_from_files(\n",
        "# #     model=vit_model,\n",
        "# #     file_list=file_list,\n",
        "# #     labels_list=labels_list,\n",
        "# #     output_path=FEATURES_DIR / \"vit_pure\" / \"train_features.npy\",\n",
        "# #     batch_size=32,\n",
        "# #     input_size=224\n",
        "# # )\n",
        "\n",
        "print(\"Exemplo de uso preparado. Descomente e ajuste conforme necess√°rio.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
