{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga8bykLItc-w"
      },
      "source": [
        "# Feature Extraction TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6eT-7r_tT8T"
      },
      "outputs": [],
      "source": [
        "# feature_extraction.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from transformers import TFAutoModel, AutoFeatureExtractor\n",
        "\n",
        "# Mapeamento de categorias conforme o paper\n",
        "MAP_CATEGORIES = {\n",
        "    'A1': 0, 'L1': 1, 'P1': 2, 'G1': 3,   # Antrum\n",
        "    'A2': 4, 'L2': 5, 'P2': 6, 'G2': 7,\n",
        "    'A3': 8, 'L3': 9, 'P3': 10, 'G3': 11,\n",
        "    'A4': 12, 'L4': 13, 'P4': 14, 'G4': 15,\n",
        "    'A5': 16, 'L5': 17, 'P5': 18,\n",
        "    'A6': 19, 'L6': 20, 'P6': 21,\n",
        "    'OTHERCLASS': 22\n",
        "}\n",
        "\n",
        "def load_and_process_csv(official_split, label_column):\n",
        "    \"\"\"\n",
        "    Carrega o CSV com a divisão oficial, substitui os rótulos usando MAP_CATEGORIES\n",
        "    e remove entradas sem rótulo.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(official_split):\n",
        "        print(\"Arquivo de split oficial não encontrado.\")\n",
        "        sys.exit(1)\n",
        "    df = pd.read_csv(official_split, index_col=0)\n",
        "    df[label_column] = df[label_column].replace(MAP_CATEGORIES).astype('Int64')\n",
        "    df.dropna(subset=[label_column], inplace=True)\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "    \"\"\"\n",
        "    Lê uma imagem do caminho informado, converte para RGB, redimensiona para (input_size, input_size)\n",
        "    usando interpolação Lanczos e retorna a imagem.\n",
        "    \"\"\"\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [input_size, input_size], method=tf.image.ResizeMethod.LANCZOS3)\n",
        "    return image\n",
        "\n",
        "def build_dataset(df, data_dir, input_size, label_column, batch_size):\n",
        "    \"\"\"\n",
        "    Cria um tf.data.Dataset a partir do DataFrame.\n",
        "    Assume que há uma coluna 'filename' com os nomes dos arquivos e a coluna de rótulo.\n",
        "    \"\"\"\n",
        "    filepaths = df['filename'].apply(lambda x: os.path.join(data_dir, x)).values\n",
        "    labels = df[label_column].values.astype(np.int32)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "\n",
        "    def _load_and_preprocess(path, label):\n",
        "        image = preprocess_image(path, input_size)\n",
        "        return image, label\n",
        "\n",
        "    ds = ds.map(_load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def build_feature_extractor(model_name, input_size):\n",
        "    \"\"\"\n",
        "    Constrói um modelo que extrai features fixas usando o ViT Base do HuggingFace.\n",
        "    Os pesos do ViT são congelados. O modelo retorna o vetor correspondente ao token [CLS].\n",
        "    \"\"\"\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "    vit_model = TFAutoModel.from_pretrained(model_name)\n",
        "    vit_model.trainable = False  # Congela o modelo\n",
        "\n",
        "    def vit_preprocessing(x):\n",
        "        # Converte os pixels para float32 e normaliza para [0,1]\n",
        "        x = tf.cast(x, tf.float32) / 255.0\n",
        "        # Aplica a normalização com base nos parâmetros do feature extractor\n",
        "        mean = tf.constant(feature_extractor.image_mean, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "        std = tf.constant(feature_extractor.image_std, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "        return (x - mean) / std\n",
        "\n",
        "    inputs = layers.Input(shape=(input_size, input_size, 3), dtype=tf.float32)\n",
        "    x = vit_preprocessing(inputs)\n",
        "\n",
        "    # O ViT espera um argumento 'pixel_values'\n",
        "    def call_vit(x):\n",
        "        outputs = vit_model(pixel_values=x)\n",
        "        return outputs.last_hidden_state[:, 0]  # vetor do token [CLS]\n",
        "\n",
        "    features = layers.Lambda(call_vit, name=\"vit_features\")(x)\n",
        "    feature_extractor_model = models.Model(inputs=inputs, outputs=features)\n",
        "    return feature_extractor_model\n",
        "\n",
        "def extract_features(model, dataset):\n",
        "    \"\"\"\n",
        "    Passa o dataset pelo modelo de extração e retorna dois arrays NumPy:\n",
        "    - features: vetor de features para cada imagem.\n",
        "    - labels: vetor de rótulos correspondentes.\n",
        "    \"\"\"\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    for batch_images, batch_labels in dataset:\n",
        "        batch_features = model.predict(batch_images)\n",
        "        features_list.append(batch_features)\n",
        "        labels_list.append(batch_labels.numpy())\n",
        "    features_np = np.concatenate(features_list, axis=0)\n",
        "    labels_np = np.concatenate(labels_list, axis=0)\n",
        "    return features_np, labels_np\n",
        "\n",
        "def main():\n",
        "    # Parâmetros de configuração\n",
        "    MODEL_NAME = \"google/vit-base-patch16-224\"  # ViT Base do HuggingFace\n",
        "    INPUT_SIZE = 224\n",
        "    BATCH_SIZE = 30\n",
        "\n",
        "    # Caminhos – ajuste conforme necessário (verifique se os caminhos estão corretos no Colab)\n",
        "    DATA_DIR = os.path.join(\"..\", \"..\", \"data\", \"Labeled Images\")\n",
        "    OFFICIAL_SPLIT = os.path.join(\"..\", \"..\", \"data\", \"official_splits\", \"image_classification.csv\")\n",
        "    OUTPUT_DIR = os.path.join(\"..\", \"output\", \"vit_features\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    LABEL_COLUMN = \"Complete agreement\"\n",
        "\n",
        "    # Carrega e processa o CSV\n",
        "    data_csv = load_and_process_csv(OFFICIAL_SPLIT, LABEL_COLUMN)\n",
        "\n",
        "    # Segue a divisão oficial (por exemplo, assumindo que 'set_type' contém 'Train' e 'Validation')\n",
        "    train_df = data_csv[data_csv['set_type'] == 'Train']\n",
        "    test_df = data_csv[data_csv['set_type'] == 'Validation']  # ou 'Test', conforme disponível\n",
        "\n",
        "    print(\"Total de imagens:\", len(data_csv))\n",
        "    print(\"Imagens de treino:\", len(train_df))\n",
        "    print(\"Imagens de teste:\", len(test_df))\n",
        "\n",
        "    # Cria os datasets do TensorFlow\n",
        "    train_ds = build_dataset(train_df, DATA_DIR, INPUT_SIZE, LABEL_COLUMN, BATCH_SIZE)\n",
        "    test_ds = build_dataset(test_df, DATA_DIR, INPUT_SIZE, LABEL_COLUMN, BATCH_SIZE)\n",
        "\n",
        "    # Constrói o modelo de extração de features (ViT congelado)\n",
        "    feature_extractor_model = build_feature_extractor(MODEL_NAME, INPUT_SIZE)\n",
        "    feature_extractor_model.summary()\n",
        "\n",
        "    # Extrai as features para os conjuntos de treino e teste\n",
        "    train_features, train_labels = extract_features(feature_extractor_model, train_ds)\n",
        "    test_features, test_labels = extract_features(feature_extractor_model, test_ds)\n",
        "\n",
        "    print(\"Shape das features de treino:\", train_features.shape)\n",
        "    print(\"Shape dos rótulos de treino:\", train_labels.shape)\n",
        "    print(\"Shape das features de teste:\", test_features.shape)\n",
        "    print(\"Shape dos rótulos de teste:\", test_labels.shape)\n",
        "\n",
        "    # Salva os arrays NumPy\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"train_features.npy\"), train_features)\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"train_labels.npy\"), train_labels)\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"test_features.npy\"), test_features)\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"test_labels.npy\"), test_labels)\n",
        "\n",
        "    print(\"Arquivos NumPy salvos em:\", OUTPUT_DIR)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MYMuBwqwsbd"
      },
      "source": [
        "# EndoscopyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loBPUBMUw4TZ"
      },
      "outputs": [],
      "source": [
        "# classification_pipeline.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Mapeamento de categorias conforme o paper\n",
        "MAP_CATEGORIES = {\n",
        "    'A1': 0, 'L1': 1, 'P1': 2, 'G1': 3,    # Antrum\n",
        "    'A2': 4, 'L2': 5, 'P2': 6, 'G2': 7,\n",
        "    'A3': 8, 'L3': 9, 'P3': 10, 'G3': 11,\n",
        "    'A4': 12, 'L4': 13, 'P4': 14, 'G4': 15,\n",
        "    'A5': 16, 'L5': 17, 'P5': 18,\n",
        "    'A6': 19, 'L6': 20, 'P6': 21,\n",
        "    'OTHERCLASS': 22\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Dataset Module\n",
        "# ------------------------------------------------------------------------------\n",
        "class EndoscopyDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Assumindo que o CSV possui uma coluna 'filename' e 'num_patient' (para stratificação)\n",
        "        img_path = os.path.join(self.data_dir, self.df.loc[idx, 'filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.df.loc[idx, 'Complete agreement']\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Lightning Module for Classification\n",
        "# ------------------------------------------------------------------------------\n",
        "class ClassificationModule(pl.LightningModule):\n",
        "    def __init__(self, model, nb_classes, class_weights, lr):\n",
        "        \"\"\"\n",
        "        model: modelo base pré-treinado com uma cabeça de classificação (por exemplo, ViT com head)\n",
        "        nb_classes: número de classes\n",
        "        class_weights: tensor de pesos para cada classe (para a loss)\n",
        "        lr: learning rate inicial\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.nb_classes = nb_classes\n",
        "        self.class_weights = class_weights\n",
        "        self.lr = lr\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        logits = self(images)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        logits = self(images)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        return {\"val_loss\": loss, \"preds\": preds, \"labels\": labels}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        preds = torch.cat([x[\"preds\"] for x in outputs])\n",
        "        labels = torch.cat([x[\"labels\"] for x in outputs])\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        labels_np = labels.cpu().numpy()\n",
        "        f1 = f1_score(labels_np, preds_np, average='macro')\n",
        "        precision = precision_score(labels_np, preds_np, average='macro')\n",
        "        recall = recall_score(labels_np, preds_np, average='macro')\n",
        "        self.log(\"val_f1_macro\", f1, prog_bar=True)\n",
        "        self.log(\"val_precision\", precision)\n",
        "        self.log(\"val_recall\", recall)\n",
        "        # Também pode registrar a acurácia, etc.\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def freeze_base(self):\n",
        "        # Congela todos os parâmetros, exceto a cabeça de classificação\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.requires_grad = False\n",
        "        # Supondo que a camada classificadora esteja em 'head'\n",
        "        if hasattr(self.model, 'head'):\n",
        "            for param in self.model.head.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            # Se a cabeça estiver sob outro nome, adapte aqui\n",
        "            raise AttributeError(\"Modelo não possui atributo 'head' para treinar.\")\n",
        "\n",
        "    def unfreeze_last_layers(self, n_layers):\n",
        "        # Método específico para ViT: descongela os últimos n_layers do transformer\n",
        "        # A implementação depende da arquitetura; a seguir um exemplo genérico:\n",
        "        # Vamos supor que o modelo tenha um atributo 'blocks' que é uma lista de camadas.\n",
        "        if hasattr(self.model, 'blocks'):\n",
        "            for block in self.model.blocks[-n_layers:]:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "        else:\n",
        "            raise AttributeError(\"Modelo não possui atributo 'blocks' para unfreeze.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # Parâmetros gerais\n",
        "    DATA_DIR = os.path.join(\"..\", \"..\", \"data\", \"Labeled Images\")\n",
        "    OFFICIAL_SPLIT = os.path.join(\"..\", \"..\", \"data\", \"official_splits\", \"image_classification.csv\")\n",
        "    OUTPUT_DIR = os.path.join(\"..\", \"output\", \"Complete agreement_40\", \"vit_base_patch16_224\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    NB_CLASSES = 23\n",
        "    LR_WARMUP = 0.001\n",
        "    LR_FINETUNE = 0.0007\n",
        "    NUM_EPOCHS_WARMUP = 10\n",
        "    NUM_EPOCHS_FINETUNE = 100\n",
        "    EARLY_STOPPING = 10\n",
        "    UNFROZEN_LAYERS = 4  # Exemplo: descongela os últimos 4 blocos do ViT\n",
        "\n",
        "    # Transformação das imagens conforme paper (redimensionamento para 224 e normalização específica)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224), interpolation=Image.LANCZOS),\n",
        "        transforms.ToTensor(),\n",
        "        # Normalização: utilizar médias e desvios do ImageNet se aplicável ou os do paper\n",
        "        transforms.Normalize([0.5990, 0.3664, 0.2769], [0.2847, 0.2190, 0.1772])\n",
        "    ])\n",
        "\n",
        "    # Carrega e processa o CSV\n",
        "    if not os.path.exists(OFFICIAL_SPLIT):\n",
        "        print(\"CSV oficial não encontrado.\")\n",
        "        sys.exit(1)\n",
        "    data_csv = pd.read_csv(OFFICIAL_SPLIT, index_col=0)\n",
        "    data_csv['Complete agreement'] = data_csv['Complete agreement'].replace(MAP_CATEGORIES).astype('Int64')\n",
        "    data_csv.dropna(subset=['Complete agreement'], inplace=True)\n",
        "    data_csv.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Separa os dados por set_type (Train, Validation, Test)\n",
        "    train_df = data_csv[data_csv['set_type'] == 'Train']\n",
        "    valid_df = data_csv[data_csv['set_type'] == 'Validation']\n",
        "    test_df = data_csv[data_csv['set_type'] == 'Test'] if 'Test' in data_csv.columns else None\n",
        "\n",
        "    print(\"Total de imagens:\", len(data_csv))\n",
        "    print(\"Imagens de treino:\", len(train_df))\n",
        "    print(\"Imagens de validação:\", len(valid_df))\n",
        "    if test_df is not None:\n",
        "        print(\"Imagens de teste:\", len(test_df))\n",
        "\n",
        "    train_dataset = EndoscopyDataset(train_df, DATA_DIR, transform=transform)\n",
        "    valid_dataset = EndoscopyDataset(valid_df, DATA_DIR, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=30, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=30, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Calcula os pesos das classes\n",
        "    class_labels = train_df['Complete agreement'].values.astype(np.int32)\n",
        "    classes = np.unique(class_labels)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=class_labels)\n",
        "    class_weights_tensor = torch.Tensor(weights).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    print(\"Pesos das classes:\", class_weights_tensor)\n",
        "\n",
        "    # Inicialize o modelo ViT (usando, por exemplo, timm ou uma implementação customizada)\n",
        "    # Aqui, assume-se que você tem uma função initialize_model que retorna (model, CNN_family)\n",
        "    # Exemplo:\n",
        "    from finetuning_models import frozen_vit  # função para manipular ViT\n",
        "    from initialize_models import initialize_model\n",
        "\n",
        "    model_ft, CNN_family = initialize_model(\"vit_base_patch16_224\", NB_CLASSES, pretrained=True, input_size=(224, 224))\n",
        "\n",
        "    # Fase 1: Warm-up – treine apenas a cabeça\n",
        "    classification_module = ClassificationModule(model=model_ft, nb_classes=NB_CLASSES,\n",
        "                                                 class_weights=class_weights_tensor,\n",
        "                                                 lr=LR_WARMUP)\n",
        "    classification_module.freeze_base()  # congela a base, treina somente a cabeça\n",
        "\n",
        "    trainer_warmup = pl.Trainer(\n",
        "        max_epochs=NUM_EPOCHS_WARMUP,\n",
        "        devices=1 if torch.cuda.is_available() else 0,\n",
        "        accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
        "        callbacks=[EarlyStopping(monitor=\"val_f1_macro\", patience=EARLY_STOPPING, mode=\"max\")]\n",
        "    )\n",
        "    trainer_warmup.fit(classification_module, train_loader, valid_loader)\n",
        "    trained_warmup_model = classification_module.model\n",
        "\n",
        "    # Fase 2: Fine-tuning – descongela os últimos blocos e treina novamente\n",
        "    # Atualize o módulo para usar LR de fine-tuning\n",
        "    classification_module.lr = LR_FINETUNE\n",
        "    classification_module.unfreeze_last_layers(UNFROZEN_LAYERS)\n",
        "\n",
        "    # Callback para salvar o melhor modelo (baseado em val_f1_macro)\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_f1_macro\",\n",
        "        dirpath=OUTPUT_DIR,\n",
        "        filename=\"best_model-val_f1_macro\",\n",
        "        save_top_k=1,\n",
        "        mode=\"max\"\n",
        "    )\n",
        "    trainer_finetune = pl.Trainer(\n",
        "        max_epochs=NUM_EPOCHS_FINETUNE,\n",
        "        devices=1 if torch.cuda.is_available() else 0,\n",
        "        accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
        "        callbacks=[EarlyStopping(monitor=\"val_f1_macro\", patience=EARLY_STOPPING, mode=\"max\"), checkpoint_callback]\n",
        "    )\n",
        "    trainer_finetune.fit(classification_module, train_loader, valid_loader)\n",
        "\n",
        "    # Após o treinamento, você pode usar o modelo final para realizar inferências e calcular métricas finais\n",
        "    print(\"Treinamento finalizado. Melhor modelo salvo em:\", checkpoint_callback.best_model_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N00kIYNPTvJ6"
      },
      "source": [
        "# Endoscopy 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6gyPLBTwBN"
      },
      "source": [
        "### Ativar cuda pra GPU se estiver ativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPpey2Z8UGVx",
        "outputId": "c7212567-6c54-465f-ed8d-7cc728297a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU está ativa\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "# Verifique se a GPU está ativa\n",
        "import tensorflow as tf\n",
        "is_cuda = len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "# Habilitar mixed precision\n",
        "# Verificar GPU e configurar precisão mista\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "else:\n",
        "    policy = tf.keras.mixed_precision.Policy('float32')\n",
        "set_global_policy(policy)\n",
        "\n",
        "\n",
        "if is_cuda:\n",
        "  print(\"GPU está ativa\")\n",
        "else:\n",
        "  print(\"GPU não está ativa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vl_yH9mVpwC"
      },
      "source": [
        "### Baixar arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9hUOKmzVsP5"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "import os,shutil\n",
        "\n",
        "\n",
        "\n",
        "def extract_tar_gz_contents(input_dir: str, output_dir: str):\n",
        "    \"\"\"Extrai o conteúdo dos arquivos .tar.gz de um diretório para outro.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "#os.path.exists(\"bruto\") and shutil.rmtree(\"./bruto\")\n",
        "\n",
        "#vit_features\n",
        "extract_tar_gz_contents('./vit_features', './vit_features')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdyVFzMiXOLG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/drive/My Drive/Mestrado 2024/Projetos/Datasets/Labeled Images.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(path=\"./bruto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjgD7lOT2hqf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#store the path to your root directory\n",
        "base='./bruto'\n",
        "\n",
        "# traverse root directory, and list directories as dirs and files as files\n",
        "for root, dirs, files in os.walk(base):\n",
        "    path = root.split(os.sep)\n",
        "\n",
        "    for file in files:\n",
        "        if not os.path.isdir(file):\n",
        "\n",
        "            # move file from nested folder into the base folder\n",
        "            shutil.move(os.path.join(root,file),os.path.join(base,file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHMlO87GffWW",
        "outputId": "4bfc03af-f8c2-4df0-c266-55a287e9149b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import Union\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "def clear_directory_folders(directory_path: Union[str, Path]) -> list:\n",
        "    \"\"\"Irreversibly removes all folders (and their content) in the specified\n",
        "    directory. Doesn't remove files of that specified directory. Returns a\n",
        "    list with folder paths Python lacks permission to delete.\"\"\"\n",
        "    erroneous_paths = []\n",
        "    for path_location in Path(directory_path).iterdir():\n",
        "        if path_location.is_dir():\n",
        "            try:\n",
        "                shutil.rmtree(path_location)\n",
        "            except PermissionError:\n",
        "                erroneous_paths.append(path_location)\n",
        "    return erroneous_paths\n",
        "\n",
        "clear_directory_folders(r'./bruto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ4tAOymUbov"
      },
      "source": [
        "# Funções"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCQPiK5NTwzN"
      },
      "outputs": [],
      "source": [
        "# feature_extraction_generic.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, models\n",
        "from transformers import TFAutoModel, AutoFeatureExtractor\n",
        "\n",
        "# Mapeamento de classes\n",
        "MAP_CATEGORIES = {\n",
        "    'A1': 0, 'L1': 1, 'P1': 2, 'G1': 3,\n",
        "    'A2': 4, 'L2': 5, 'P2': 6, 'G2': 7,\n",
        "    'A3': 8, 'L3': 9, 'P3': 10, 'G3': 11,\n",
        "    'A4': 12, 'L4': 13, 'P4': 14, 'G4': 15,\n",
        "    'A5': 16, 'L5': 17, 'P5': 18,\n",
        "    'A6': 19, 'L6': 20, 'P6': 21,\n",
        "    'OTHERCLASS': 22\n",
        "}\n",
        "\n",
        "def load_and_process_csv(official_split, label_column):\n",
        "    \"\"\"Carrega e processa o CSV com os rótulos mapeados.\"\"\"\n",
        "    if not os.path.exists(official_split):\n",
        "        print(\"Arquivo de split oficial não encontrado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = pd.read_csv(official_split, index_col=0)\n",
        "    df[label_column] = df[label_column].replace(MAP_CATEGORIES).astype('Int64')\n",
        "    df.dropna(subset=[label_column], inplace=True)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def preprocess_image(image_path, input_size, resample_method):\n",
        "    \"\"\"Pré-processamento da imagem com redimensionamento dinâmico.\"\"\"\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3) # Ensure image is divisible by patch size (16 for DeiT-base-distilled-patch16-224)\n",
        "\n",
        "    return tf.image.resize(\n",
        "        image, [input_size, input_size],\n",
        "        method=resample_method\n",
        "    )\n",
        "\n",
        "def build_dataset(df, data_dir, input_size, label_column, batch_size, resample_method):\n",
        "    \"\"\"Constrói dataset com parâmetros específicos do modelo.\"\"\"\n",
        "    filepaths = df['filename'].apply(lambda x: os.path.join(data_dir, x)).values\n",
        "    labels = df[label_column].values.astype(np.int32)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "\n",
        "    def _load_preprocess(path, label):\n",
        "        image = preprocess_image(path, input_size, resample_method)\n",
        "        return image, label\n",
        "\n",
        "    return (ds\n",
        "           .map(_load_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "           .batch(batch_size)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ViTFeatureExtractor(layers.Layer):\n",
        "    def __init__(self, feature_extractor, vit_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.vit_model = vit_model\n",
        "        self._hidden_size = vit_model.config.hidden_size\n",
        "\n",
        "        # Configurar parâmetros de normalização como variáveis do layer\n",
        "        self.mean = self.add_weight(\n",
        "            name='mean',\n",
        "            shape=(1, 1, 1, 3),\n",
        "            initializer=tf.constant_initializer(feature_extractor.image_mean),\n",
        "            trainable=False\n",
        "        )\n",
        "        self.std = self.add_weight(\n",
        "            name='std',\n",
        "            shape=(1, 1, 1, 3),\n",
        "            initializer=tf.constant_initializer(feature_extractor.image_std),\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Conversão de tipo e normalização\n",
        "        x = tf.cast(inputs, tf.float32)\n",
        "        x = (x / 255.0 - self.mean) / self.std\n",
        "\n",
        "        # Converter para formato channels-first\n",
        "        x = tf.transpose(x, perm=[0, 3, 1, 2])\n",
        "\n",
        "        # Chamada do modelo com verificação de tipo explícita\n",
        "        outputs = self.vit_model(pixel_values=x)\n",
        "\n",
        "        # Extrair o token [CLS] e garantir que é um tensor\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        return tf.ensure_shape(cls_token, [None, self._hidden_size])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'hidden_size': self._hidden_size,\n",
        "            'feature_extractor': self.feature_extractor,\n",
        "            'vit_model': self.vit_model\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Necessário para serialização correta\n",
        "        return cls(**config)\n",
        "\n",
        "def build_feature_extractor(feature_extractor, vit_model, input_size):\n",
        "    inputs = layers.Input(shape=(input_size, input_size, 3), dtype=tf.float32)  # Tipo explícito\n",
        "    features = ViTFeatureExtractor(feature_extractor, vit_model)(inputs)\n",
        "    return models.Model(inputs=inputs, outputs=features)\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(model, dataset):\n",
        "    # Separar features e labels\n",
        "    features_dataset = dataset.map(lambda x, y: x)\n",
        "    features = model.predict(features_dataset)\n",
        "    labels = np.concatenate([y.numpy() for _, y in dataset], axis=0)\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aO431r4dW28"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Configurações\n",
        "    MODEL_NAMES = [\n",
        "        \"google/vit-base-patch16-224\",\n",
        "        \"google/vit-large-patch32-384\",\n",
        "        \"facebook/deit-base-distilled-patch16-224\",\n",
        "        \"facebook/deit-base-patch16-384\",\n",
        "        \"facebook/vit-mae-base\",\n",
        "        \"facebook/vit-mae-large\"\n",
        "    ]\n",
        "    BATCH_SIZE = 64\n",
        "    DATA_DIR = \"./bruto\"\n",
        "    OFFICIAL_SPLIT = \"/content/drive/My Drive/Mestrado 2024/Projetos/Datasets/official_splits/image_classification.csv\"\n",
        "    BASE_OUTPUT_DIR = \"./vit_features\"\n",
        "    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Carregar dados\n",
        "    df = load_and_process_csv(OFFICIAL_SPLIT, \"Complete agreement\")\n",
        "    train_df = df[df['set_type'] == 'Train']\n",
        "    validation_df = df[df['set_type'] == 'Validation']\n",
        "    test_df = df[df['set_type'] == 'Test']\n",
        "    # stats dataset\n",
        "    print(f\"Train dataset size : {train_df.value_counts()}\")\n",
        "    print(f\"Validation dataset size : {validation_df.value_counts()}\")\n",
        "    print(f\"Test dataset size : {test_df.value_counts()}\")\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        print(f\"\\nProcessando modelo: {model_name}\")\n",
        "\n",
        "        # Carregar configurações do modelo\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "        vit_model = TFAutoModel.from_pretrained(model_name)\n",
        "        vit_model.trainable = False\n",
        "\n",
        "        # Determinar parâmetros dinâmicos\n",
        "        if 'deit' in model_name.lower():\n",
        "            #input_size = (224 // 16) * 16  # Fixed to 224 for this specific DeiT model\n",
        "            input_size = feature_extractor.size[\"height\"]\n",
        "        else:\n",
        "            input_size = feature_extractor.size[\"height\"]\n",
        "        resample_method = tf.image.ResizeMethod.BILINEAR  # Mapear conforme necessário\n",
        "\n",
        "        # Construir datasets\n",
        "        train_ds = build_dataset(train_df, DATA_DIR, input_size, \"Complete agreement\", BATCH_SIZE, resample_method)\n",
        "        validation_ds = build_dataset(validation_df, DATA_DIR, input_size, \"Complete agreement\", BATCH_SIZE, resample_method)\n",
        "        test_ds = build_dataset(test_df, DATA_DIR, input_size, \"Complete agreement\", BATCH_SIZE, resample_method)\n",
        "\n",
        "        # stats dataset\n",
        "        print(f\"Train dataset size after build_dataset: {len(train_ds)}\")\n",
        "        print(f\"Test dataset size after build_dataset: {len(test_ds)}\")\n",
        "        # Construir extrator de features\n",
        "        model = build_feature_extractor(feature_extractor, vit_model, input_size)\n",
        "        model.build(input_shape=(None, input_size, input_size, 3))\n",
        "\n",
        "         # Verificação adicional para DeiT\n",
        "        if 'deit' in model_name.lower():\n",
        "            # Forçar inicialização do modelo\n",
        "            dummy_input = tf.ones((1, input_size, input_size, 3))\n",
        "            _ = model(dummy_input)\n",
        "        # Extrair features\n",
        "        train_features = model.predict(train_ds.map(lambda x, y: x))\n",
        "        validation_features = model.predict(validation_ds.map(lambda x, y: x))\n",
        "        test_features = model.predict(test_ds.map(lambda x, y: x))\n",
        "        train_labels = np.concatenate([y.numpy() for _, y in train_ds], axis=0)\n",
        "        validation_labels = np.concatenate([y.numpy() for _, y in validation_ds], axis=0)\n",
        "        test_labels = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
        "\n",
        "        # Salvar resultados\n",
        "        model_slug = model_name.replace(\"/\", \"_\")\n",
        "        output_dir = os.path.join(BASE_OUTPUT_DIR, model_slug)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(output_dir, \"train_features.npy\"), train_features)\n",
        "        np.save(os.path.join(output_dir, \"train_labels.npy\"), train_labels)\n",
        "        np.save(os.path.join(output_dir, \"validation_features.npy\"), validation_features)\n",
        "        np.save(os.path.join(output_dir, \"validation_labels.npy\"), validation_labels)\n",
        "        np.save(os.path.join(output_dir, \"test_features.npy\"), test_features)\n",
        "        np.save(os.path.join(output_dir, \"test_labels.npy\"), test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a15a8887f1d04390b503a4ba009017cf",
            "e89147e203ca4f65bdde1fd1718d7879",
            "0c8c00d5846343f1a4520296fd100d1d",
            "bd9588c74c1d4a2d8d21b0f771ded41f",
            "673b0cab021f46bfabfe9369bfc2b6d4",
            "91d85e65ce7d487fba3e45419850013a",
            "6665420a059b450890d1f5658c5966dc",
            "2fe17d96831c4ff0977c42212d5a0b2f",
            "328ed8af04634360b5850d146774285b",
            "966e2828d024496e8ac4354394e2e471",
            "57444da2c9d345fcbb2b4f4f4c477e33",
            "628331b142fe4f4390cf246a7caf266e",
            "5813027cd0b14b5cac40830fdc78980b",
            "62f52298483f48489b2359a1d67cbdee",
            "c1901cfc3da64407b1b61a8ab4b22e0d",
            "3a9d9e05821f4edf9b5d2e2aa5e8bcad",
            "9fbf0980a211403aa59a5128b6604650",
            "38061b3da2974259ac6502e5e6e0dfbb",
            "ea8c13119dd04278a580d732ecc9b7c9",
            "1ce3cef6ff714ad28999cbda615aed8c",
            "2562139611c048929124166a9908b052",
            "84ffe9c0393d4c26900ca920f70fc050",
            "9ec6021a7c12461692a24f7a3d012112",
            "1e4f65e546f64f439cfdd56d66bc581c",
            "6f69dc0545ed412ab95b722c3fe29c41",
            "30ae025a60f94a76a7f2e53f82ded2f9",
            "36e6bc91ac8f4d59b577acf0f7161281",
            "e7eda4d940b243b9a33f63f86896afa0",
            "546a62f009bc4d179fa8cac261bb2aab",
            "35aa881b24184597bf6e3f28b68a6830",
            "94995d425b364ff1befcd423838ab107",
            "a4132ce398e14bf3a017006a185defa5",
            "e3a494b79d454329bd9a423866956436"
          ]
        },
        "id": "oL_SAYXvdYnq",
        "outputId": "9204f339-3d06-490d-ff21-5d75d5cad0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando...\n",
            "Train dataset size : num patient  filename                                  FG1 (Team A)  FG2 (Team A)  G1 (Team B)  G2 (Team B)  Complete agreement  Triple agreement  FG agreement  G agreement  FG1-G1 agreement  FG1-G2 agreement  FG2-G1 agreement  FG2-G2 agreement  set_type\n",
            "387          c5d311b4-7d13-45fb-965c-e18e392f99ff.jpg  G4            G4            G4           G4           15                  G4                G4            G4           G4                G4                G4                G4                Train       1\n",
            "1            00770fe2-9158-468b-bc3b-4a1ff6fd2fea.jpg  OTHERCLASS    OTHERCLASS    OTHERCLASS   OTHERCLASS   22                  OTHERCLASS        OTHERCLASS    OTHERCLASS   OTHERCLASS        OTHERCLASS        OTHERCLASS        OTHERCLASS        Train       1\n",
            "             2de64b53-a39e-4832-82ce-f7e9ac179ec9.jpg  P2            P2            P2           P2           6                   P2                P2            P2           P2                P2                P2                P2                Train       1\n",
            "             5252d061-9f21-44e4-ab6a-9ca93e81dd4b.jpg  P5            P5            P5           P5           18                  P5                P5            P5           P5                P5                P5                P5                Train       1\n",
            "             71fc4fc5-08b4-4075-b6bd-fc1f102d8e74.jpg  L3            L3            L3           L3           9                   L3                L3            L3           L3                L3                L3                L3                Train       1\n",
            "                                                                                                                                                                                                                                                                 ..\n",
            "5            a12aa5ea-ff4e-46f6-a769-b2b9ed9901f6.jpg  P6            P6            P6           P6           21                  P6                P6            P6           P6                P6                P6                P6                Train       1\n",
            "             b7180896-30da-4c46-b2c8-aedf35476459.jpg  OTHERCLASS    OTHERCLASS    OTHERCLASS   OTHERCLASS   22                  OTHERCLASS        OTHERCLASS    OTHERCLASS   OTHERCLASS        OTHERCLASS        OTHERCLASS        OTHERCLASS        Train       1\n",
            "             c1cf2983-978c-4af6-88e8-b7bcc12fbb67.jpg  G3            G3            G3           G3           11                  G3                G3            G3           G3                G3                G3                G3                Train       1\n",
            "             cbca1cfb-9143-44f0-9edd-25db0d61f648.jpg  P4            P4            P4           P4           14                  P4                P4            P4           P4                P4                P4                P4                Train       1\n",
            "             d86ef07b-204e-4554-a8c1-a35ef9115cae.jpg  G1            G1            G1           G1           3                   G1                G1            G1           G1                G1                G1                G1                Train       1\n",
            "Name: count, Length: 3722, dtype: int64\n",
            "Validation dataset size : num patient  filename                                  FG1 (Team A)  FG2 (Team A)  G1 (Team B)  G2 (Team B)  Complete agreement  Triple agreement  FG agreement  G agreement  FG1-G1 agreement  FG1-G2 agreement  FG2-G1 agreement  FG2-G2 agreement  set_type  \n",
            "386          df9edee1-ff65-434a-b035-55eaf8b6158d.jpg  P6            P6            P6           P6           21                  P6                P6            P6           P6                P6                P6                P6                Validation    1\n",
            "3            0642b1af-6339-4e86-889f-02c0e093d2b4.jpg  P3            P3            P3           P3           10                  P3                P3            P3           P3                P3                P3                P3                Validation    1\n",
            "             27286800-0b87-45e6-92f8-9904e339f814.jpg  G4            G4            G4           G4           15                  G4                G4            G4           G4                G4                G4                G4                Validation    1\n",
            "             30f3c5a1-a8b6-4851-880a-c62c9e77cf9a.jpg  L2            L2            L2           L2           5                   L2                L2            L2           L2                L2                L2                L2                Validation    1\n",
            "             4c299a19-3463-43b3-804b-d56d9bc8fb18.jpg  A6            A6            A6           A6           19                  A6                A6            A6           A6                A6                A6                A6                Validation    1\n",
            "                                                                                                                                                                                                                                                                   ..\n",
            "14           729c3f98-f737-4312-ba6c-3a42c81bc3e1.jpg  G3            G3            G3           G3           11                  G3                G3            G3           G3                G3                G3                G3                Validation    1\n",
            "             9eafe86d-d34c-46d3-9c0c-56673f6b88a9.jpg  L2            L2            L2           L2           5                   L2                L2            L2           L2                L2                L2                L2                Validation    1\n",
            "             a4c378f6-94a3-4ec7-81e4-3e963efcb67a.jpg  A6            A6            A6           A6           19                  A6                A6            A6           A6                A6                A6                A6                Validation    1\n",
            "             b0110eaf-9281-48d7-8544-ce24536ddc2b.jpg  A1            A1            A1           A1           0                   A1                A1            A1           A1                A1                A1                A1                Validation    1\n",
            "             b1eff3d3-4a20-44c9-8e1a-56b6e2ef5237.jpg  L1            L1            L1           L1           1                   L1                L1            L1           L1                L1                L1                L1                Validation    1\n",
            "Name: count, Length: 793, dtype: int64\n",
            "Test dataset size : num patient  filename                                  FG1 (Team A)  FG2 (Team A)  G1 (Team B)  G2 (Team B)  Complete agreement  Triple agreement  FG agreement  G agreement  FG1-G1 agreement  FG1-G2 agreement  FG2-G1 agreement  FG2-G2 agreement  set_type\n",
            "385          edec1b0b-738e-453f-8bdc-62f1f919ac7b.jpg  A4            A4            A4           A4           12                  A4                A4            A4           A4                A4                A4                A4                Test        1\n",
            "2            19fa935c-21d1-4479-b9b8-1e958a80cb2e.jpg  L3            L3            L3           L3           9                   L3                L3            L3           L3                L3                L3                L3                Test        1\n",
            "             2079de00-526e-4214-8784-a19396a429ee.jpg  L5            L5            L5           L5           17                  L5                L5            L5           L5                L5                L5                L5                Test        1\n",
            "381          9bdca677-7d71-4e58-8d53-f2a9606141c3.jpg  P6            P6            P6           P6           21                  P6                P6            P6           P6                P6                P6                P6                Test        1\n",
            "             b33ab751-e569-4c8f-aaf5-69354266ca92.jpg  L6            L6            L6           L6           20                  L6                L6            L6           L6                L6                L6                L6                Test        1\n",
            "                                                                                                                                                                                                                                                                 ..\n",
            "2            a798857c-bd1a-4f23-99fa-95d319983475.jpg  A3            A3            A3           A3           8                   A3                A3            A3           A3                A3                A3                A3                Test        1\n",
            "             b9bea692-f9d7-4ab3-87b2-e8917976e094.jpg  L2            L2            L2           L2           5                   L2                L2            L2           L2                L2                L2                L2                Test        1\n",
            "             ba191c04-4283-45aa-a3b1-abd544e1d138.jpg  OTHERCLASS    OTHERCLASS    OTHERCLASS   OTHERCLASS   22                  OTHERCLASS        OTHERCLASS    OTHERCLASS   OTHERCLASS        OTHERCLASS        OTHERCLASS        OTHERCLASS        Test        1\n",
            "             bc242716-85ce-46d5-8534-fecd4fc2706f.jpg  G4            G4            G4           G4           15                  G4                G4            G4           G4                G4                G4                G4                Test        1\n",
            "             d2b9980a-91cd-44f7-a97a-cb29622cd0d1.jpg  G3            G3            G3           G3           11                  G3                G3            G3           G3                G3                G3                G3                Test        1\n",
            "Name: count, Length: 803, dtype: int64\n",
            "\n",
            "Processando modelo: facebook/vit-mae-large\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-35-f9c1b630bacb>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[label_column] = df[label_column].replace(MAP_CATEGORIES).astype('Int64')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a15a8887f1d04390b503a4ba009017cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "628331b142fe4f4390cf246a7caf266e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ec6021a7c12461692a24f7a3d012112",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at facebook/vit-mae-large were not used when initializing TFViTMAEModel: ['decoder']\n",
            "- This IS expected if you are initializing TFViTMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFViTMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFViTMAEModel were initialized from the model checkpoint at facebook/vit-mae-large.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTMAEModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size after build_dataset: 59\n",
            "Test dataset size after build_dataset: 13\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 322ms/step\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 605ms/step\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 631ms/step\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"Iniciando...\")\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxVj4XgRZsvk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Defina os caminhos para os arquivos de features e labels\n",
        "OUTPUT_DIR = os.path.join(\"..\", \"./output\", \"/content/drive/My Drive/Mestrado 2024/Projetos/Datasets/vit_features_gastroVision/google_vit-large-patch32-384\")\n",
        "TRAIN_FEATURES_PATH = os.path.join(OUTPUT_DIR, \"train_features.npy\")\n",
        "TRAIN_LABELS_PATH   = os.path.join(OUTPUT_DIR, \"train_labels.npy\")\n",
        "TEST_FEATURES_PATH  = os.path.join(OUTPUT_DIR, \"test_features.npy\")\n",
        "TEST_LABELS_PATH    = os.path.join(OUTPUT_DIR, \"test_labels.npy\")\n",
        "\n",
        "# Carrega os arrays NumPy\n",
        "train_features = np.load(TRAIN_FEATURES_PATH)\n",
        "train_labels   = np.load(TRAIN_LABELS_PATH)\n",
        "test_features  = np.load(TEST_FEATURES_PATH)\n",
        "test_labels    = np.load(TEST_LABELS_PATH)\n",
        "\n",
        "print(\"Shape das features de treino:\", train_features.shape)\n",
        "print(\"Shape dos rótulos de treino:\", train_labels.shape)\n",
        "print(\"Shape das features de teste:\", test_features.shape)\n",
        "print(\"Shape dos rótulos de teste:\", test_labels.shape)\n",
        "\n",
        "# Define o classificador SVM com kernel linear e pesos balanceados\n",
        "svm = SVC(kernel='linear', class_weight='balanced')\n",
        "\n",
        "# Define os parâmetros para busca em grid (valores de C)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Usa GridSearchCV com validação cruzada de 5 folds para selecionar o melhor hiperparâmetro\n",
        "grid_search = GridSearchCV(svm, param_grid, scoring='f1_macro', cv=5, n_jobs=-1)\n",
        "grid_search.fit(train_features, train_labels)\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Seleciona o melhor modelo SVM\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Faz predições no conjunto de teste\n",
        "predictions = best_svm.predict(test_features)\n",
        "\n",
        "# Calcula métricas de desempenho\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "f1_macro = f1_score(test_labels, predictions, average='macro')\n",
        "\n",
        "print(\"Acurácia no conjunto de teste:\", accuracy)\n",
        "print(\"F1 Macro no conjunto de teste:\", f1_macro)\n",
        "#print(\"\\nRelatório de Classificação:\\n\", classification_report(test_labels, predictions,output_dict=True))\n",
        "\n",
        "report = classification_report(test_labels, predictions, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "print(\"\\nRelatório de Classificação:\\n\", df_report)\n",
        "\n",
        "# Salva o relatório de classificação em um arquivo CSV\n",
        "df_report.to_csv('./output/classification_report.csv', index=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlqhls3xp7xA",
        "outputId": "ac9aeb48-84af-4ef9-8b44-827a657b356d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12.390625"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "793/64"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ga8bykLItc-w",
        "4MYMuBwqwsbd"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c8c00d5846343f1a4520296fd100d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe17d96831c4ff0977c42212d5a0b2f",
            "max": 217,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_328ed8af04634360b5850d146774285b",
            "value": 217
          }
        },
        "1ce3cef6ff714ad28999cbda615aed8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4f65e546f64f439cfdd56d66bc581c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7eda4d940b243b9a33f63f86896afa0",
            "placeholder": "​",
            "style": "IPY_MODEL_546a62f009bc4d179fa8cac261bb2aab",
            "value": "tf_model.h5: 100%"
          }
        },
        "2562139611c048929124166a9908b052": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe17d96831c4ff0977c42212d5a0b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ae025a60f94a76a7f2e53f82ded2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4132ce398e14bf3a017006a185defa5",
            "placeholder": "​",
            "style": "IPY_MODEL_e3a494b79d454329bd9a423866956436",
            "value": " 1.32G/1.32G [00:54&lt;00:00, 23.8MB/s]"
          }
        },
        "328ed8af04634360b5850d146774285b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35aa881b24184597bf6e3f28b68a6830": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e6bc91ac8f4d59b577acf0f7161281": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38061b3da2974259ac6502e5e6e0dfbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a9d9e05821f4edf9b5d2e2aa5e8bcad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546a62f009bc4d179fa8cac261bb2aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57444da2c9d345fcbb2b4f4f4c477e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5813027cd0b14b5cac40830fdc78980b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbf0980a211403aa59a5128b6604650",
            "placeholder": "​",
            "style": "IPY_MODEL_38061b3da2974259ac6502e5e6e0dfbb",
            "value": "config.json: 100%"
          }
        },
        "628331b142fe4f4390cf246a7caf266e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5813027cd0b14b5cac40830fdc78980b",
              "IPY_MODEL_62f52298483f48489b2359a1d67cbdee",
              "IPY_MODEL_c1901cfc3da64407b1b61a8ab4b22e0d"
            ],
            "layout": "IPY_MODEL_3a9d9e05821f4edf9b5d2e2aa5e8bcad"
          }
        },
        "62f52298483f48489b2359a1d67cbdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea8c13119dd04278a580d732ecc9b7c9",
            "max": 677,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ce3cef6ff714ad28999cbda615aed8c",
            "value": 677
          }
        },
        "6665420a059b450890d1f5658c5966dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673b0cab021f46bfabfe9369bfc2b6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f69dc0545ed412ab95b722c3fe29c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35aa881b24184597bf6e3f28b68a6830",
            "max": 1318856784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94995d425b364ff1befcd423838ab107",
            "value": 1318856784
          }
        },
        "84ffe9c0393d4c26900ca920f70fc050": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d85e65ce7d487fba3e45419850013a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94995d425b364ff1befcd423838ab107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "966e2828d024496e8ac4354394e2e471": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ec6021a7c12461692a24f7a3d012112": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e4f65e546f64f439cfdd56d66bc581c",
              "IPY_MODEL_6f69dc0545ed412ab95b722c3fe29c41",
              "IPY_MODEL_30ae025a60f94a76a7f2e53f82ded2f9"
            ],
            "layout": "IPY_MODEL_36e6bc91ac8f4d59b577acf0f7161281"
          }
        },
        "9fbf0980a211403aa59a5128b6604650": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15a8887f1d04390b503a4ba009017cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e89147e203ca4f65bdde1fd1718d7879",
              "IPY_MODEL_0c8c00d5846343f1a4520296fd100d1d",
              "IPY_MODEL_bd9588c74c1d4a2d8d21b0f771ded41f"
            ],
            "layout": "IPY_MODEL_673b0cab021f46bfabfe9369bfc2b6d4"
          }
        },
        "a4132ce398e14bf3a017006a185defa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd9588c74c1d4a2d8d21b0f771ded41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966e2828d024496e8ac4354394e2e471",
            "placeholder": "​",
            "style": "IPY_MODEL_57444da2c9d345fcbb2b4f4f4c477e33",
            "value": " 217/217 [00:00&lt;00:00, 25.4kB/s]"
          }
        },
        "c1901cfc3da64407b1b61a8ab4b22e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2562139611c048929124166a9908b052",
            "placeholder": "​",
            "style": "IPY_MODEL_84ffe9c0393d4c26900ca920f70fc050",
            "value": " 677/677 [00:00&lt;00:00, 86.3kB/s]"
          }
        },
        "e3a494b79d454329bd9a423866956436": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7eda4d940b243b9a33f63f86896afa0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e89147e203ca4f65bdde1fd1718d7879": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d85e65ce7d487fba3e45419850013a",
            "placeholder": "​",
            "style": "IPY_MODEL_6665420a059b450890d1f5658c5966dc",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "ea8c13119dd04278a580d732ecc9b7c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
