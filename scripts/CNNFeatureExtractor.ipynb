{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class\n"
      ],
      "metadata": {
        "id": "AM0j8_dym4IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "# Verifique se a GPU está ativa\n",
        "import tensorflow as tf\n",
        "is_cuda = len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "# Habilitar mixed precision\n",
        "# Verificar GPU e configurar precisão mista\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "else:\n",
        "    policy = tf.keras.mixed_precision.Policy('float32')\n",
        "set_global_policy(policy)\n",
        "\n",
        "\n",
        "if is_cuda:\n",
        "  print(\"GPU está ativa\")\n",
        "else:\n",
        "  print(\"GPU não está ativa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rrSvvK8m7Gg",
        "outputId": "57ec77ad-3c9c-41cf-cfc3-c0a630097f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU está ativa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import os,shutil\n",
        "\n",
        "\n",
        "\n",
        "def extract_tar_gz_contents(input_dir: str, output_dir: str):\n",
        "    \"\"\"Extrai o conteúdo dos arquivos .tar.gz de um diretório para outro.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "#os.path.exists(\"bruto\") and shutil.rmtree(\"./bruto\")\n",
        "\n",
        "#cnn_features\n",
        "extract_tar_gz_contents('./cnn_features', './cnn_features')\n",
        "\n"
      ],
      "metadata": {
        "id": "3pipR4Sim_O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/drive/My Drive/Mestrado 2024/Projetos/Datasets/Labeled Images.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(path=\"./bruto\")"
      ],
      "metadata": {
        "id": "RO7eIczCnB_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#store the path to your root directory\n",
        "base='./bruto'\n",
        "\n",
        "# traverse root directory, and list directories as dirs and files as files\n",
        "for root, dirs, files in os.walk(base):\n",
        "    path = root.split(os.sep)\n",
        "\n",
        "    for file in files:\n",
        "        if not os.path.isdir(file):\n",
        "\n",
        "            # move file from nested folder into the base folder\n",
        "            shutil.move(os.path.join(root,file),os.path.join(base,file))"
      ],
      "metadata": {
        "id": "oab8SeaGnD_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "def clear_directory_folders(directory_path: Union[str, Path]) -> list:\n",
        "    \"\"\"Irreversibly removes all folders (and their content) in the specified\n",
        "    directory. Doesn't remove files of that specified directory. Returns a\n",
        "    list with folder paths Python lacks permission to delete.\"\"\"\n",
        "    erroneous_paths = []\n",
        "    for path_location in Path(directory_path).iterdir():\n",
        "        if path_location.is_dir():\n",
        "            try:\n",
        "                shutil.rmtree(path_location)\n",
        "            except PermissionError:\n",
        "                erroneous_paths.append(path_location)\n",
        "    return erroneous_paths\n",
        "\n",
        "clear_directory_folders(r'./bruto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lhu5aBTnGa6",
        "outputId": "021a4b27-00ab-4504-b799-d6a09104ff3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lalUodQVmSiD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152,EfficientNetB4\n",
        "from transformers import TFConvNextModel, ConvNextFeatureExtractor\n",
        "\n",
        "# Mapeamento de classes\n",
        "MAP_CATEGORIES = {\n",
        "    'A1': 0, 'L1': 1, 'P1': 2, 'G1': 3,\n",
        "    'A2': 4, 'L2': 5, 'P2': 6, 'G2': 7,\n",
        "    'A3': 8, 'L3': 9, 'P3': 10, 'G3': 11,\n",
        "    'A4': 12, 'L4': 13, 'P4': 14, 'G4': 15,\n",
        "    'A5': 16, 'L5': 17, 'P5': 18,\n",
        "    'A6': 19, 'L6': 20, 'P6': 21,\n",
        "    'OTHERCLASS': 22\n",
        "}\n",
        "\n",
        "def load_and_process_csv(official_split, label_column):\n",
        "    \"\"\"Carrega e processa o CSV com os rótulos mapeados.\"\"\"\n",
        "    if not os.path.exists(official_split):\n",
        "        print(\"Arquivo de split oficial não encontrado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = pd.read_csv(official_split, index_col=0)\n",
        "    df[label_column] = df[label_column].replace(MAP_CATEGORIES).astype('Int64')\n",
        "    df.dropna(subset=[label_column], inplace=True)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def preprocess_image(image_path, input_size, resample_method):\n",
        "    \"\"\"Pré-processamento da imagem com redimensionamento dinâmico.\"\"\"\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3) # Ensure image is divisible by patch size (16 for DeiT-base-distilled-patch16-224)\n",
        "\n",
        "    return tf.image.resize(\n",
        "        image, [input_size, input_size],\n",
        "        method=resample_method\n",
        "    )\n",
        "\n",
        "def build_dataset(df, data_dir, input_size, label_column, batch_size, resample_method):\n",
        "    \"\"\"Constrói dataset com parâmetros específicos do modelo.\"\"\"\n",
        "    filepaths = df['filename'].apply(lambda x: os.path.join(data_dir, x)).values\n",
        "    labels = df[label_column].values.astype(np.int32)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "\n",
        "    def _load_preprocess(path, label):\n",
        "        image = preprocess_image(path, input_size, resample_method)\n",
        "        return image, label\n",
        "\n",
        "    return (ds\n",
        "           .map(_load_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "           .batch(batch_size)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "\n",
        "class CNNFeatureExtractor(layers.Layer):\n",
        "    def __init__(self, model_name='resnet50', include_top=False, pooling='avg', **kwargs):\n",
        "        \"\"\"\n",
        "        Constrói um extrator de features baseado em CNNs pré-treinadas.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Nome do modelo CNN pré-treinado.\n",
        "            include_top (bool): Se True, inclui as camadas fully connected do modelo.\n",
        "            pooling (str): Tipo de pooling para a extração de features ('avg' ou 'max').\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "        self.include_top = include_top\n",
        "        self.pooling = pooling\n",
        "\n",
        "        # Carrega o modelo CNN pré-treinado\n",
        "        if self.model_name == 'resnet50':\n",
        "            self.cnn_model = ResNet50(include_top=self.include_top, weights='imagenet', pooling=self.pooling)\n",
        "        elif self.model_name == 'resnet101':\n",
        "            self.cnn_model = ResNet101(include_top=self.include_top, weights='imagenet', pooling=self.pooling)\n",
        "        elif self.model_name == 'resnet152':\n",
        "            self.cnn_model = ResNet152(include_top=self.include_top, weights='imagenet', pooling=self.pooling)\n",
        "        elif self.model_name == 'efficientnetb4':\n",
        "            self.cnn_model = EfficientNetB4(include_top=self.include_top, weights='imagenet', pooling=self.pooling)\n",
        "        elif self.model_name.startswith('facebook/convnext'):\n",
        "            # Carrega o modelo ConvNeXt usando transformers\n",
        "            self.feature_extractor = ConvNextFeatureExtractor.from_pretrained(model_name)\n",
        "            self.cnn_model = TFConvNextModel.from_pretrained(model_name)\n",
        "            self.cnn_model.trainable = False\n",
        "            #self.hidden_size = self.cnn_model.config.hidden_size\n",
        "            self.hidden_size = self.cnn_model.config.hidden_sizes[-1]  # Usa o último valor de hidden_sizes\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Modelo {model_name} não suportado.\")\n",
        "\n",
        "        # Congela o modelo para evitar treinamento\n",
        "        if not self.model_name.startswith('facebook/'):\n",
        "            self.cnn_model.trainable = False\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Extrai features das imagens de entrada usando o modelo CNN.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Tensor de imagens com shape (batch_size, height, width, channels).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Features extraídas com shape (batch_size, feature_dim).\n",
        "        \"\"\"\n",
        "        if self.model_name.startswith('facebook/'):\n",
        "            # Pré-processamento específico para ConvNeXt\n",
        "            inputs = tf.cast(inputs, tf.float32) / 255.0\n",
        "            mean = tf.constant(self.feature_extractor.image_mean, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "            std = tf.constant(self.feature_extractor.image_std, shape=[1, 1, 1, 3], dtype=tf.float32)\n",
        "            inputs = (inputs - mean) / std\n",
        "            inputs = tf.transpose(inputs, perm=[0, 3, 1, 2])  # channels-first\n",
        "            outputs = self.cnn_model(pixel_values=inputs)\n",
        "            features = tf.reduce_mean(outputs.last_hidden_state, axis=1)  # Pooling global médio\n",
        "        else:\n",
        "            # Pré-processamento para modelos Keras (ResNet, EfficientNet)\n",
        "            if self.model_name == 'resnet50':\n",
        "                inputs = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "            elif self.model_name == 'resnet101':\n",
        "                inputs = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "            elif self.model_name == 'resnet152':\n",
        "                inputs = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "            elif self.model_name == 'efficientnetb4':\n",
        "                inputs = tf.keras.applications.efficientnet.preprocess_input(inputs)\n",
        "            features = self.cnn_model(inputs)\n",
        "\n",
        "        # Ensure features is a Tensor, not a TensorSpec\n",
        "        features = tf.convert_to_tensor(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Define a forma de saída da camada.\n",
        "        \"\"\"\n",
        "        if self.model_name.startswith('facebook/'):\n",
        "            # Para ConvNeXt, a saída é (batch_size, hidden_size)\n",
        "            return (input_shape[0], self.hidden_size)\n",
        "        else:\n",
        "            # Para modelos Keras, a forma de saída depende do pooling\n",
        "            if self.pooling == 'avg':\n",
        "                return (input_shape[0], 2048)  # ResNet/EfficientNet\n",
        "            else:\n",
        "                return (input_shape[0], 2048)  # Outros casos\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Retorna a configuração da camada para serialização.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'model_name': self.model_name,\n",
        "            'include_top': self.include_top,\n",
        "            'pooling': self.pooling\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        \"\"\"Recria a instância da camada a partir da configuração.\"\"\"\n",
        "        return cls(**config)\n",
        "\n",
        "def extract_features(model, dataset):\n",
        "    # Separar features e labels\n",
        "    features_dataset = dataset.map(lambda x, y: x)\n",
        "    features = model.predict(features_dataset)\n",
        "    labels = np.concatenate([y.numpy() for _, y in dataset], axis=0)\n",
        "     # Verifique a forma do array de features\n",
        "    print(f\"Shape of features before processing: {features.shape}\")\n",
        "    # Remova dimensões extras se necessário\n",
        "    if len(features.shape) > 2:\n",
        "        # Achatar as dimensões (7, 7) em uma única dimensão de tamanho 49\n",
        "        features = tf.reshape(features, (features.shape[0], -1))  # Resulta em (3722, 49)\n",
        "\n",
        "    print(f\"Shape of features after processing: {features.shape}\")\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def build_cnn_feature_extractor(model_name='resnet50', input_size=(224, 224, 3)):\n",
        "    \"\"\"\n",
        "    Constrói um modelo de extração de features usando uma CNN pré-treinada.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Nome do modelo CNN.\n",
        "        input_size (tuple): Tamanho da imagem de entrada (height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Modelo de extração de features.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_size, dtype=tf.float32)\n",
        "    features = CNNFeatureExtractor(model_name=model_name)(inputs)\n",
        "    return models.Model(inputs=inputs, outputs=features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_1PhqoxEWpfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso da classe"
      ],
      "metadata": {
        "id": "bOq6kHstmoTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso\n",
        "# https://huggingface.co/facebook/convnext-large-224\n",
        "\n",
        "def main():\n",
        "    # Configurações\n",
        "    MODEL_NAMES = [\n",
        "    #'resnet152',\n",
        "    #'resnet50',\n",
        "    #'resnet101',\n",
        "    #'facebook/convnext-base-384',\n",
        "    #'efficientnetb4',\n",
        "    #'facebook/convnext-large-224-22k-1k'\n",
        "    'facebook/convnext-large-224'\n",
        "]\n",
        "    BATCH_SIZE = 32\n",
        "    INPUT_SIZE = (224, 224, 3)  # Tamanho da imagem de entrada\n",
        "    DATA_DIR = \"./bruto\"\n",
        "    OFFICIAL_SPLIT = \"/content/drive/My Drive/Mestrado 2024/Projetos/Datasets/official_splits/image_classification.csv\"\n",
        "    BASE_OUTPUT_DIR = \"./cnn_features\"\n",
        "    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Carregar dados\n",
        "    df = load_and_process_csv(OFFICIAL_SPLIT, \"Complete agreement\")\n",
        "    train_df = df[df['set_type'] == 'Train']\n",
        "    validation_df = df[df['set_type'] == 'Validation']\n",
        "    test_df = df[df['set_type'] == 'Test']\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        print(f\"\\nProcessando modelo: {model_name}\")\n",
        "\n",
        "        # Construir datasets\n",
        "        train_ds = build_dataset(train_df, DATA_DIR, INPUT_SIZE[0], \"Complete agreement\", BATCH_SIZE, tf.image.ResizeMethod.BILINEAR)\n",
        "        validation_ds = build_dataset(validation_df, DATA_DIR, INPUT_SIZE[0], \"Complete agreement\", BATCH_SIZE, tf.image.ResizeMethod.BILINEAR)\n",
        "        test_ds = build_dataset(test_df, DATA_DIR, INPUT_SIZE[0], \"Complete agreement\", BATCH_SIZE, tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "        # Define o tamanho de entrada com base no modelo\n",
        "        if '384' in model_name:\n",
        "            input_size = (384, 384, 3)\n",
        "        else:\n",
        "            input_size = (224, 224, 3)\n",
        "\n",
        "        # Construir extrator de features\n",
        "        model = build_cnn_feature_extractor(model_name=model_name, input_size=input_size)\n",
        "        model.build(input_shape=(None, *input_size))\n",
        "\n",
        "        # Extrair features\n",
        "        train_features, train_labels = extract_features(model, train_ds)\n",
        "        validation_features, validation_labels = extract_features(model, validation_ds)\n",
        "        test_features, test_labels = extract_features(model, test_ds)\n",
        "\n",
        "        # Salvar resultados\n",
        "        output_dir = os.path.join(BASE_OUTPUT_DIR, model_name)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(output_dir, \"train_features.npy\"), train_features)\n",
        "        np.save(os.path.join(output_dir, \"train_labels.npy\"), train_labels)\n",
        "        np.save(os.path.join(output_dir, \"validation_features.npy\"), validation_features)\n",
        "        np.save(os.path.join(output_dir, \"validation_labels.npy\"), validation_labels)\n",
        "        np.save(os.path.join(output_dir, \"test_features.npy\"), test_features)\n",
        "        np.save(os.path.join(output_dir, \"test_labels.npy\"), test_labels)"
      ],
      "metadata": {
        "id": "s6m0FNiLmoem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "      print(\"Iniciando...\")\n",
        "      main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va2h4t2SpUYf",
        "outputId": "71953af3-1a52-4de2-ab9f-0b802c7ae563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando...\n",
            "\n",
            "Processando modelo: facebook/convnext-large-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a24416c33a82>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[label_column] = df[label_column].replace(MAP_CATEGORIES).astype('Int64')\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/convnext/feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some layers from the model checkpoint at facebook/convnext-large-224 were not used when initializing TFConvNextModel: ['classifier']\n",
            "- This IS expected if you are initializing TFConvNextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFConvNextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFConvNextModel were initialized from the model checkpoint at facebook/convnext-large-224.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFConvNextModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 215ms/step\n",
            "Shape of features before processing: (3722, 7, 7)\n",
            "Shape of features after processing: (3722, 49)\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 680ms/step\n",
            "Shape of features before processing: (793, 7, 7)\n",
            "Shape of features after processing: (793, 49)\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 564ms/step\n",
            "Shape of features before processing: (803, 7, 7)\n",
            "Shape of features after processing: (803, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "extrator_name = \"cnn_features\"\n",
        "!zip -r {extrator_name}.zip ./cnn_features  # Changed the directory to './cnn_features'\n",
        "files.download(f'./{extrator_name}.zip')"
      ],
      "metadata": {
        "id": "6Rb3cvWwtqcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "8edf8fbb-31ac-4986-d865-51eaee89cf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: cnn_features/ (stored 0%)\n",
            "  adding: cnn_features/facebook/ (stored 0%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/ (stored 0%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/test_features.npy (deflated 10%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/train_features.npy (deflated 10%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/validation_features.npy (deflated 10%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/validation_labels.npy (deflated 81%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/train_labels.npy (deflated 85%)\n",
            "  adding: cnn_features/facebook/convnext-large-224/test_labels.npy (deflated 81%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_763cc427-3f63-46a8-a6e7-91720acc31d3\", \"cnn_features.zip\", 945535)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}