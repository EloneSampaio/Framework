{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avalia√ß√£o Estat√≠stica - Compara√ß√£o dos 5 Bra√ßos Experimentais\n",
        "\n",
        "Este notebook realiza an√°lise estat√≠stica comparativa dos resultados dos 5 bra√ßos experimentais usando:\n",
        "\n",
        "- **Teste de Friedman:** Para verificar se h√° diferen√ßas significativas entre os bra√ßos\n",
        "- **P√≥s-testes:** Nemenyi, Conover e Bonferroni para identificar quais bra√ßos diferem significativamente\n",
        "- **Visualiza√ß√µes:** Rankings, heatmaps e diagramas de diferen√ßa cr√≠tica\n",
        "\n",
        "## Bra√ßos Comparados\n",
        "\n",
        "1. Baseline CNN\n",
        "2. ViT Puro\n",
        "3. ViT + Contrastive\n",
        "4. ViT + MIM\n",
        "5. ViT + Sparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy.stats import friedmanchisquare, chi2, t, norm\n",
        "from scikit_posthocs import posthoc_conover, posthoc_dunn, posthoc_nemenyi_friedman\n",
        "\n",
        "# MLflow para rastreamento de experimentos\n",
        "import mlflow\n",
        "from mlflow import log_metric, log_param, log_artifacts\n",
        "\n",
        "# ============================================\n",
        "# DETEC√á√ÉO DE AMBIENTE (COLAB OU LOCAL)\n",
        "# ============================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Colab detectado - Drive montado\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚úÖ Ambiente local detectado\")\n",
        "\n",
        "# Configurar caminhos baseado no ambiente\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = Path(\"/content/drive/MyDrive/Mestrado_TCC\")\n",
        "    RESULTS_DIR = BASE_DIR / \"results\" / \"classifications\"\n",
        "    EVAL_DIR = BASE_DIR / \"results\" / \"evaluations\"\n",
        "    MLRUNS_DIR = BASE_DIR / \"mlruns\"\n",
        "    # Mudar para diret√≥rio do framework\n",
        "    FRAMEWORK_DIR = BASE_DIR / \"Framework\"\n",
        "    if FRAMEWORK_DIR.exists():\n",
        "        os.chdir(FRAMEWORK_DIR)\n",
        "else:\n",
        "    BASE_DIR = Path(\"../\")\n",
        "    RESULTS_DIR = BASE_DIR / \"results\" / \"classifications\"\n",
        "    EVAL_DIR = BASE_DIR / \"results\" / \"evaluations\"\n",
        "    MLRUNS_DIR = BASE_DIR / \"mlruns\"\n",
        "\n",
        "# Criar diret√≥rios\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configura√ß√£o do MLflow\n",
        "MLRUNS_DIR.mkdir(exist_ok=True)\n",
        "mlflow.set_tracking_uri(str(MLRUNS_DIR.absolute()))\n",
        "\n",
        "# Lista dos bra√ßos experimentais\n",
        "EXPERIMENTAL_ARMS = [\n",
        "    \"baseline_cnn\",\n",
        "    \"vit_pure\",\n",
        "    \"vit_contrastive\",\n",
        "    \"vit_mim\",\n",
        "    \"vit_sparse\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüìÅ Diret√≥rios configurados:\")\n",
        "print(f\"   Resultados: {RESULTS_DIR}\")\n",
        "print(f\"   Avalia√ß√µes: {EVAL_DIR}\")\n",
        "print(f\"   MLflow: {MLRUNS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classe StatisticalAnalysis\n",
        "\n",
        "Classe adaptada do notebook original para an√°lise estat√≠stica dos bra√ßos experimentais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StatisticalAnalysis:\n",
        "    def __init__(self, f1_scores_dict, alpha=0.05):\n",
        "        \"\"\"\n",
        "        Classe para realizar Teste de Friedman e P√≥s-Testes de Diferen√ßa Cr√≠tica.\n",
        "        \n",
        "        Par√¢metros:\n",
        "        - f1_scores_dict: dicion√°rio contendo os nomes dos bra√ßos e seus F1-scores por fold.\n",
        "        - alpha: n√≠vel de signific√¢ncia estat√≠stica (padr√£o: 0.05).\n",
        "        \"\"\"\n",
        "        self.f1_scores_dict = f1_scores_dict\n",
        "        self.models = list(f1_scores_dict.keys())\n",
        "        self.results_array = np.array(list(f1_scores_dict.values()))\n",
        "        self.alpha = alpha\n",
        "        self.N = len(next(iter(f1_scores_dict.values())))  # N√∫mero de folds\n",
        "        self.k = len(self.models)  # N√∫mero de bra√ßos\n",
        "        self.df_friedman = self.k - 1\n",
        "        \n",
        "        # Valores cr√≠ticos\n",
        "        self.valor_critico_friedman = chi2.ppf(1 - self.alpha, self.df_friedman)\n",
        "        self.q_nemenyi = self.get_q_nemenyi(self.k)\n",
        "        self.cd_nemenyi = self.q_nemenyi * np.sqrt(self.k * (self.k + 1) / (6 * self.N))\n",
        "        \n",
        "        print(f\"N√∫mero de experimentos (N) = {self.N}\")\n",
        "        print(f\"N√∫mero de bra√ßos (k) = {self.k}\")\n",
        "        print(f\"Valor cr√≠tico de Friedman (Œ±={self.alpha}) = {self.valor_critico_friedman:.3f}\")\n",
        "        print(f\"Diferen√ßa Cr√≠tica (CD) para Nemenyi = {self.cd_nemenyi:.3f}\")\n",
        "\n",
        "    def get_q_nemenyi(self, k):\n",
        "        \"\"\"Obt√©m o valor cr√≠tico q_0.05 da tabela do p√≥s-teste de Nemenyi.\"\"\"\n",
        "        nemenyi_table = {\n",
        "            2: 1.960, 3: 2.343, 4: 2.569, 5: 2.728, 6: 2.850,\n",
        "            7: 2.949, 8: 3.031, 9: 3.102, 10: 3.164\n",
        "        }\n",
        "        return nemenyi_table.get(k, 3.164)\n",
        "\n",
        "    def friedman_test(self):\n",
        "        \"\"\"Executa o Teste de Friedman.\"\"\"\n",
        "        statistic, p_value = friedmanchisquare(*self.results_array)\n",
        "        print(f\"\\nTeste de Friedman (F1-score):\")\n",
        "        print(f\"  Estat√≠stica = {statistic:.4f}\")\n",
        "        print(f\"  p-valor = {p_value:.4f}\")\n",
        "        \n",
        "        if p_value < self.alpha:\n",
        "            print(f\"  **Rejeitamos H0** (p < {self.alpha}): H√° diferen√ßa significativa entre os bra√ßos\")\n",
        "        else:\n",
        "            print(f\"  **N√£o rejeitamos H0** (p >= {self.alpha}): N√£o h√° diferen√ßa significativa\")\n",
        "        \n",
        "        return p_value\n",
        "\n",
        "    def nemenyi_test(self):\n",
        "        \"\"\"Executa o P√≥s-Teste de Nemenyi.\"\"\"\n",
        "        df_results = pd.DataFrame(self.results_array.T, columns=self.models)\n",
        "        posthoc_res = posthoc_nemenyi_friedman(df_results)\n",
        "        print(\"\\nP√≥s-Teste de Nemenyi:\\n\", posthoc_res)\n",
        "        self.plot_cd_diagram(posthoc_res, \"Nemenyi\")\n",
        "        return posthoc_res\n",
        "\n",
        "    def plot_cd_diagram(self, posthoc_matrix, test_type=\"Nemenyi\"):\n",
        "        \"\"\"Gera um Diagrama de Diferen√ßa Cr√≠tica (CD Diagram).\"\"\"\n",
        "        f1_means = {model: np.mean(scores) for model, scores in self.f1_scores_dict.items()}\n",
        "        rankings = sorted(f1_means.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title(f\"Diagrama de Diferen√ßa Cr√≠tica ({test_type})\", fontsize=14, fontweight='bold')\n",
        "        \n",
        "        y_pos = 1\n",
        "        for i, (model, score) in enumerate(rankings):\n",
        "            plt.scatter(score, y_pos, color=\"black\", s=100, zorder=3)\n",
        "            plt.text(score, y_pos + 0.15, model.replace('_', ' ').title(), \n",
        "                    ha='center', fontsize=10, fontweight='bold')\n",
        "            y_pos += 1\n",
        "        \n",
        "        # Linha de Diferen√ßa Cr√≠tica\n",
        "        if len(rankings) > 0:\n",
        "            plt.plot([rankings[0][1] - self.cd_nemenyi, rankings[0][1] + self.cd_nemenyi], \n",
        "                    [y_pos - 1.5, y_pos - 1.5], color=\"red\", linewidth=2, linestyle='--', zorder=2)\n",
        "            plt.text(rankings[0][1], y_pos - 1.2, f\"CD = {self.cd_nemenyi:.3f}\", \n",
        "                    ha='center', fontsize=10, fontweight=\"bold\", color='red')\n",
        "        \n",
        "        plt.xlabel(\"F1-Score M√©dio\", fontsize=12)\n",
        "        plt.ylabel(\"Ranking dos Bra√ßos\", fontsize=12)\n",
        "        plt.yticks([])\n",
        "        plt.grid(axis='x', linestyle=\"--\", alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EVAL_DIR / \"cd_diagram.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def posthoc_tests(self, method=\"conover\"):\n",
        "        \"\"\"Executa os p√≥s-testes de Conover ou Bonferroni.\"\"\"\n",
        "        df_results = pd.DataFrame(self.f1_scores_dict)\n",
        "        df_results = df_results.melt(var_name='group', value_name='value')\n",
        "        \n",
        "        if method == \"conover\":\n",
        "            posthoc_res = posthoc_conover(df_results, val_col='value', group_col='group', p_adjust=\"holm\")\n",
        "            title = \"P√≥s-Teste de Conover (Holm) - F1-score\"\n",
        "        elif method == \"bonferroni\":\n",
        "            posthoc_res = posthoc_dunn(df_results, val_col='value', group_col='group', p_adjust=\"bonferroni\")\n",
        "            title = \"P√≥s-Teste de Dunn (Bonferroni) - F1-score\"\n",
        "        else:\n",
        "            raise ValueError(\"M√©todo inv√°lido. Escolha 'conover' ou 'bonferroni'.\")\n",
        "        \n",
        "        print(f\"\\n{title}:\\n\", posthoc_res)\n",
        "        self.plot_heatmap(posthoc_res, title)\n",
        "        return posthoc_res\n",
        "\n",
        "    def plot_heatmap(self, posthoc_matrix, title=\"P√≥s-Teste Estat√≠stico\"):\n",
        "        \"\"\"Gera um heatmap para visualizar os resultados do p√≥s-teste.\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(posthoc_matrix, annot=True, cmap=\"RdYlGn_r\", fmt=\".4f\", \n",
        "                   linewidths=0.5, center=0.05, vmin=0, vmax=1)\n",
        "        plt.title(title, fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EVAL_DIR / f\"heatmap_{title.lower().replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_rankings(self):\n",
        "        \"\"\"Gera um gr√°fico de ranking dos bra√ßos baseado no F1-score m√©dio.\"\"\"\n",
        "        f1_means = {model: np.mean(scores) for model, scores in self.f1_scores_dict.items()}\n",
        "        sorted_f1 = sorted(f1_means.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        colors = sns.color_palette(\"viridis\", len(sorted_f1))\n",
        "        bars = plt.barh([x[0].replace('_', ' ').title() for x in sorted_f1], \n",
        "                        [x[1] for x in sorted_f1], color=colors)\n",
        "        plt.xlabel(\"F1-Score M√©dio\", fontsize=12)\n",
        "        plt.ylabel(\"Bra√ßo Experimental\", fontsize=12)\n",
        "        plt.title(\"Ranking dos Bra√ßos Experimentais (F1-Score)\", fontsize=14, fontweight='bold')\n",
        "        \n",
        "        # Adicionar valores nas barras\n",
        "        for i, (bar, (model, score)) in enumerate(zip(bars, sorted_f1)):\n",
        "            plt.text(score, i, f' {score:.4f}', va='center', fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EVAL_DIR / \"rankings.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "print(\"Classe StatisticalAnalysis definida!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregar Resultados dos Classificadores\n",
        "\n",
        "Carrega os resultados dos classificadores SVM e SRC para cada bra√ßo experimental.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classification_results(classifier_type=\"SVM\"):\n",
        "    \"\"\"\n",
        "    Carrega resultados de classifica√ß√£o para todos os bra√ßos.\n",
        "    \n",
        "    Args:\n",
        "        classifier_type: \"SVM\" ou \"SRC\"\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dicion√°rio com F1-scores por bra√ßo\n",
        "    \"\"\"\n",
        "    f1_scores = {}\n",
        "    \n",
        "    for arm in EXPERIMENTAL_ARMS:\n",
        "        result_path = RESULTS_DIR / f\"{arm}_{classifier_type.lower()}_results.json\"\n",
        "        \n",
        "        if not result_path.exists():\n",
        "            print(f\"‚ö†Ô∏è  Resultados n√£o encontrados para {arm} - {classifier_type}\")\n",
        "            continue\n",
        "        \n",
        "        with open(result_path, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        \n",
        "        # Para an√°lise estat√≠stica, precisamos de m√∫ltiplos valores (folds)\n",
        "        # Se tiver apenas um valor, vamos usar valida√ß√£o cruzada ou replicar\n",
        "        # Por enquanto, vamos usar o valor de teste como exemplo\n",
        "        # Em um cen√°rio real, voc√™ teria m√∫ltiplos folds\n",
        "        \n",
        "        # Se os resultados tiverem m√∫ltiplos folds, use-os\n",
        "        # Caso contr√°rio, vamos simular m√∫ltiplos valores baseados no resultado √∫nico\n",
        "        test_f1 = results.get('test_f1_macro', 0)\n",
        "        \n",
        "        # Para demonstra√ß√£o, vamos criar 5 valores similares (em produ√ß√£o, use folds reais)\n",
        "        # Voc√™ pode modificar isso para usar valida√ß√£o cruzada real\n",
        "        f1_scores[arm] = [test_f1] * 5  # Simula√ß√£o - substitua por folds reais\n",
        "        \n",
        "        print(f\"‚úÖ {arm} - {classifier_type}: F1 = {test_f1:.4f}\")\n",
        "    \n",
        "    return f1_scores\n",
        "\n",
        "# Carregar resultados\n",
        "print(\"Carregando resultados SVM...\")\n",
        "svm_f1_scores = load_classification_results(\"SVM\")\n",
        "\n",
        "print(\"\\nCarregando resultados SRC...\")\n",
        "src_f1_scores = load_classification_results(\"SRC\")\n",
        "\n",
        "print(f\"\\n‚úÖ {len(svm_f1_scores)} bra√ßos com resultados SVM\")\n",
        "print(f\"‚úÖ {len(src_f1_scores)} bra√ßos com resultados SRC\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An√°lise Estat√≠stica - SVM\n",
        "\n",
        "Executa an√°lise estat√≠stica completa para os resultados do classificador SVM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(svm_f1_scores) >= 3:  # M√≠nimo de 3 bra√ßos para teste de Friedman\n",
        "    print(\"=\"*60)\n",
        "    print(\"AN√ÅLISE ESTAT√çSTICA - CLASSIFICADOR SVM\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    analysis_svm = StatisticalAnalysis(svm_f1_scores, alpha=0.05)\n",
        "    \n",
        "    # Teste de Friedman\n",
        "    p_value = analysis_svm.friedman_test()\n",
        "    \n",
        "    # Se houver diferen√ßa significativa, executar p√≥s-testes\n",
        "    if p_value < 0.05:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"P√ìS-TESTES (diferen√ßa significativa detectada)\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Teste de Nemenyi\n",
        "        nemenyi_results = analysis_svm.nemenyi_test()\n",
        "        nemenyi_results.to_csv(EVAL_DIR / \"svm_nemenyi_results.csv\")\n",
        "        \n",
        "        # Teste de Conover\n",
        "        conover_results = analysis_svm.posthoc_tests(\"conover\")\n",
        "        conover_results.to_csv(EVAL_DIR / \"svm_conover_results.csv\")\n",
        "        \n",
        "        # Visualiza√ß√µes\n",
        "        analysis_svm.plot_rankings()\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  N√£o h√° diferen√ßa significativa entre os bra√ßos (p >= 0.05)\")\n",
        "        print(\"   P√≥s-testes n√£o s√£o necess√°rios.\")\n",
        "        analysis_svm.plot_rankings()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  N√∫mero insuficiente de bra√ßos para an√°lise estat√≠stica (m√≠nimo: 3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An√°lise Estat√≠stica - SRC\n",
        "\n",
        "Executa an√°lise estat√≠stica completa para os resultados do classificador SRC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(src_f1_scores) >= 3:  # M√≠nimo de 3 bra√ßos para teste de Friedman\n",
        "    print(\"=\"*60)\n",
        "    print(\"AN√ÅLISE ESTAT√çSTICA - CLASSIFICADOR SRC\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    analysis_src = StatisticalAnalysis(src_f1_scores, alpha=0.05)\n",
        "    \n",
        "    # Teste de Friedman\n",
        "    p_value = analysis_src.friedman_test()\n",
        "    \n",
        "    # Se houver diferen√ßa significativa, executar p√≥s-testes\n",
        "    if p_value < 0.05:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"P√ìS-TESTES (diferen√ßa significativa detectada)\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Teste de Nemenyi\n",
        "        nemenyi_results = analysis_src.nemenyi_test()\n",
        "        nemenyi_results.to_csv(EVAL_DIR / \"src_nemenyi_results.csv\")\n",
        "        \n",
        "        # Teste de Conover\n",
        "        conover_results = analysis_src.posthoc_tests(\"conover\")\n",
        "        conover_results.to_csv(EVAL_DIR / \"src_conover_results.csv\")\n",
        "        \n",
        "        # Visualiza√ß√µes\n",
        "        analysis_src.plot_rankings()\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  N√£o h√° diferen√ßa significativa entre os bra√ßos (p >= 0.05)\")\n",
        "        print(\"   P√≥s-testes n√£o s√£o necess√°rios.\")\n",
        "        analysis_src.plot_rankings()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  N√∫mero insuficiente de bra√ßos para an√°lise estat√≠stica (m√≠nimo: 3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compara√ß√£o entre Classificadores\n",
        "\n",
        "Compara o desempenho de SVM vs SRC para cada bra√ßo experimental.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_classifiers():\n",
        "    \"\"\"Compara SVM vs SRC para cada bra√ßo.\"\"\"\n",
        "    comparison_data = []\n",
        "    \n",
        "    for arm in EXPERIMENTAL_ARMS:\n",
        "        svm_path = RESULTS_DIR / f\"{arm}_svm_results.json\"\n",
        "        src_path = RESULTS_DIR / f\"{arm}_src_results.json\"\n",
        "        \n",
        "        svm_f1 = None\n",
        "        src_f1 = None\n",
        "        \n",
        "        if svm_path.exists():\n",
        "            with open(svm_path, 'r') as f:\n",
        "                svm_results = json.load(f)\n",
        "                svm_f1 = svm_results.get('test_f1_macro', None)\n",
        "        \n",
        "        if src_path.exists():\n",
        "            with open(src_path, 'r') as f:\n",
        "                src_results = json.load(f)\n",
        "                src_f1 = src_results.get('test_f1_macro', None)\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Bra√ßo': arm.replace('_', ' ').title(),\n",
        "            'SVM F1': f\"{svm_f1:.4f}\" if svm_f1 else \"N/A\",\n",
        "            'SRC F1': f\"{src_f1:.4f}\" if src_f1 else \"N/A\",\n",
        "            'Melhor': 'SVM' if (svm_f1 and src_f1 and svm_f1 > src_f1) else ('SRC' if src_f1 else 'N/A')\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\nCompara√ß√£o SVM vs SRC:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Salvar\n",
        "    comparison_df.to_csv(EVAL_DIR / \"classifier_comparison.csv\", index=False)\n",
        "    \n",
        "    # Visualiza√ß√£o\n",
        "    if len(comparison_data) > 0:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        x = np.arange(len(comparison_data))\n",
        "        width = 0.35\n",
        "        \n",
        "        svm_scores = [float(d['SVM F1']) if d['SVM F1'] != 'N/A' else 0 for d in comparison_data]\n",
        "        src_scores = [float(d['SRC F1']) if d['SRC F1'] != 'N/A' else 0 for d in comparison_data]\n",
        "        \n",
        "        plt.bar(x - width/2, svm_scores, width, label='SVM', alpha=0.8)\n",
        "        plt.bar(x + width/2, src_scores, width, label='SRC', alpha=0.8)\n",
        "        \n",
        "        plt.xlabel('Bra√ßo Experimental', fontsize=12)\n",
        "        plt.ylabel('F1-Score', fontsize=12)\n",
        "        plt.title('Compara√ß√£o SVM vs SRC por Bra√ßo Experimental', fontsize=14, fontweight='bold')\n",
        "        plt.xticks(x, [d['Bra√ßo'] for d in comparison_data], rotation=45, ha='right')\n",
        "        plt.legend()\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EVAL_DIR / \"classifier_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Executar compara√ß√£o\n",
        "comparison_df = compare_classifiers()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
